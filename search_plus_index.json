{"./":{"url":"./","title":"我的 kubernetes 学习手册","keywords":"","body":"简介 Kubernetes 是一个开源的，用于管理多个主机上的容器化的应用，Kubernetes 的目标是让部署容器化的应用简单并且高效，Kubernetes 提供了应用部署、规划、更新、维护的一种机制。 Kubernetes 一个核心的特点就是能够自主的管理容器来保证平台中的容器按照用户的期望状态运行。 在 Kubenetes中，所有的容器均在 Pod 中运行，一个 Pod 可以承载一个或者多个相关的容器，同一个 Pod 中的容器会部署在同一个物理机器上并且能够共享资源。一个 Pod 也可以包含 O 个或者多个磁盘卷组（volumes），这些卷组将会以目录的形式提供给一个容器，或者被所有 Pod 中的容器共享。 Kubernetes 提供了服务的抽象，并提供了固定的 IP 地址和 DNS 名称，而这些与一系列 Pod 进行动态关联，所以我们可以关联任何我们想关联的Pod，当一个 Pod 中的容器访问这个地址的时候，这个请求会被转发到本地代理（kube proxy），每台机器上均有一个本地代理，然后被转发到相应的后端容器。Kubernetes 通过一种轮询机制选择相应的后端容器，这些动态的Pod被替换的时候，Kube proxy 时刻追踪着，所以，服务的 IP地址（dns名称），从来不变。 起源 在 Docker 作为高级容器引擎快速发展的同时，Google 也开始将自身在容器技术及集群方面的积累贡献出来。在 Google 内部，容器技术已经应用了很多年，Borg 系统运行管理着成千上万的容器应用，在它的支持下，无论是谷歌搜索、Gmail 还是谷歌地图，可以轻而易举地从庞大的数据中心中获取技术资源来支撑服务运行。 Borg 是集群的管理器，在它的系统中，运行着众多集群，而每个集群可由成千上万的服务器联接组成，Borg 每时每刻都在处理来自众多应用程序所提交的成百上千的 Job，对这些 Job 进行接收、调度、启动、停止、重启和监控。正如 Borg 论文中所说，Borg 提供了 3 大好处： 隐藏资源管理和错误处理，用户仅需要关注应用的开发。 服务高可用、高可靠。 可将负载运行在由成千上万的机器联合而成的集群中。 作为 Google 的竞争技术优势，Borg 理所当然的被视为商业秘密隐藏起来，但当 Tiwtter 的工程师精心打造出属于自己的 Borg 系统（Mesos）时，Google 也审时度势地推出了来源于自身技术理论的新的开源工具。 2014 年 6 月，谷歌云计算专家埃里克·布鲁尔（Eric Brewer）在旧金山的发布会为这款新的开源工具揭牌，它的名字 Kubernetes 在希腊语中意思是船长或领航员，这也恰好与它在容器集群管理中的作用吻合，即作为装载了集装箱（Container）的众多货船的指挥者，负担着全局调度和运行监控的职责。 虽然 Google 推出 Kubernetes 的目的之一是推广其周边的计算引擎（Google Compute Engine）和谷歌应用引擎（Google App Engine）。但 Kubernetes 的出现能让更多的互联网企业可以享受到连接众多计算机成为集群资源池的好处。 Kubernetes 对计算资源进行了更高层次的抽象，通过将容器进行细致的组合，将最终的应用服务交给用户。Kubernetes 在模型建立之初就考虑了容器跨机连接的要求，支持多种网络解决方案，同时在 Service 层次构建集群范围的 SDN 网络。其目的是将服务发现和负载均衡放置到容器可达的范围，这种透明的方式便利了各个服务间的通信，并为微服务架构的实践提供了平台基础。而在 Pod 层次上，作为 Kubernetes 可操作的最小对象，其特征更是对微服务架构的原生支持。 Kubernetes 项目来源于 Borg，可以说是集结了 Borg 设计思想的精华，并且吸收了 Borg 系统中的经验和教训。 Kubernetes 作为容器集群管理工具，于 2015 年 7 月 22 日迭代到 v1.0 并正式对外公布，这意味着这个开源容器编排系统可以正式在生产环境使用。与此同时，谷歌联合 Linux 基金会及其他合作伙伴共同成立了 CNCF 基金会(Cloud Native Computing Foundation)，并将 Kuberentes 作为首个编入 CNCF 管理体系的开源项目，助力容器技术生态的发展进步。Kubernetes 项目凝结了 Google 过去十年间在生产环境的经验和教训，从 Borg 的多任务分配资源块到 Kubernetes 的多副本 Pod，从 Borg 的 Cell 集群管理，到 Kubernetes 设计理念中的联邦集群，在 Docker 等高级引擎带动容器技术兴起和大众化的同时，为容器集群管理提供独了到见解和新思路。 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/architecture.html":{"url":"basic/architecture.html","title":"kubernetes 体系结构","keywords":"","body":"kubernetes 体系结构 设计架构 Kubernetes 集群包含有节点代理 kubelet 和 Master 组件(APIs，scheduler，etc)，一切都基于分布式的存储系统。下面这张图是 Kubernetes 的架构图。 在这张系统架构图中，我们把服务分为运行在工作节点上的服务和组成集群级别控制板的服务。 Kubernetes节点有运行应用容器必备的服务，而这些都是受 Master 的控制。 每次个节点上运行 Docker，负责所有具体的镜像下载和容器运行。 核心组件 etcd：保存整个集群的状态； apiserver：提供资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制； controller manager：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler：负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； kubelet：负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理； Container runtime：负责镜像管理以及Pod和容器的真正运行（CRI）； kube-proxy：负责为 Service 提供 cluster 内部的服务发现和负载均衡； 附加组件 除了核心组件，还有一些推荐的 Add-ons： kube-dns：负责为整个集群提供 DNS 服务 Ingress Controller：为服务提供外网入口 Heapster：提供资源监控 Dashboard：提供 GUI Federation：提供跨可用区的集群 Fluentd-elasticsearch：提供集群日志采集、存储与查询 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/api.html":{"url":"basic/api.html","title":"kubernetes API 对象","keywords":"","body":"kubernetes API 对象 API 对象是 K8s 集群中的管理操作单元。K8s 集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的 API 对象，支持对该功能的管理操作。 例如副本集 Replica Set 对应的 API 对象是 RS。 每个API对象都有3大类属性：元数据 metadata、规范 spec 和状态 status。 K8s 中所有的配置都是通过 API 对象的 spec 去设置的，也就是用户通过配置系统的理想状态来改变系统，这是 k8s 重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为 3 的操作运行多次也还是一个结果，而给副本数加 1 的操作就不是声明式的，运行多次结果就错了。 Pod K8s 有很多技术概念，同时对应很多 API 对象，最重要的也是最基础的是微服务 Pod。Pod 是在 K8s 集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod 的设计理念是支持多个容器在一个 Pod 中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod 对多容器的支持是 K8s 最基础的设计理念。比如你运行一个操作系统发行版的软件仓库，一个 Nginx 容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像有可能不是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。 Pod 是 K8s 集群中所有业务类型的基础，可以看作运行在 K8s 集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前 K8s 中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为 Deployment、Job、DaemonSet 和 PetSet。 复制控制器（Replication Controller，RC） RC 是 K8s 集群中最早的保证 Pod 高可用的 API 对象。通过监控运行中的 Pod 来保证集群中运行指定数目的 Pod 副本。指定的数目可以是多个也可以是 1 个；少于指定数目，RC 就会启动运行新的 Pod 副本；多于指定数目，RC 就会杀死多余的 Pod 副本。即使在指定数目为 1 的情况下，通过 RC 运行 Pod 也比直接运行 Pod 更明智，因为 RC 也可以发挥它高可用的能力，保证永远有 1 个 Pod 在运行。RC 是 K8s 较早期的技术概念，只适用于长期伺服型的业务类型，比如提供高可用的 Web 服务。 Replication Controller 只会对那些 RestartPolicy=Always 的 Pod 的生效。 Replication Controller 只支持基于等式的 selector（env=dev 或 environment!=qa） 副本集（Replica Set，RS） RS 是新一代 RC，提供同样的高可用能力，区别主要在于 RS 后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为 Deployment 的理想状态参数使用。 部署（Deployment） 部署表示用户对 K8s 集群的一次更新操作。部署是一个比 RS 应用模式更广的 API 对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的 RS，然后逐渐将新 RS 中副本数增加到理想状态，将旧 RS 中的副本数减小到0的复合操作；这样一个复合操作用一个 RS 是不太好描述的，所以用一个更通用的 Deployment 来描述。以 K8s 的发展方向，未来对所有长期伺服型的的业务的管理，都会通过 Deployment 来管理。 服务（Service） RC、RS 和 Deployment 只是保证了支撑服务的微服务 Pod 的数量，但是没有解决如何访问这些服务的问题。一个 Pod 只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的 IP 启动一个新的 Pod，因此不能以确定的 IP 和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在 K8s 集群中，客户端需要访问的服务就是 Service 对象。每个 Service 会对应一个集群内部有效的虚拟 IP，集群内部通过虚拟 IP 访问一个服务。在 K8s 集群中微服务的负载均衡是由 Kube-proxy 实现的。Kube-proxy 是 K8s 集群内部的负载均衡器。它是一个分布式代理服务器，在 K8s 的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的 Kube-proxy 就越多，高可用节点也随之增多。与之相比，我们平时在服务器端做个反向代理做负载均衡，还要进一步解决反向代理的负载均衡和高可用问题。 任务（Job） Job 是 K8s 用来控制批处理型任务的 API 对象。批处理业务与长期伺服业务的主要区别是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job 管理的 Pod 根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的 spec.completions 策略而不同：单 Pod 型任务有一个 Pod 成功就标志完成；定数成功型任务保证有 N 个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。 后台支撑服务集（DaemonSet） 长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的 Pod，有些节点上又没有这类 Pod 运行；而后台支撑型服务的核心关注点在 K8s 集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类 Pod 运行。节点可能是所有集群节点也可能是通过 nodeSelector 选定的一些特定节点。典型的后台支撑型服务包括，存储，日志和监控等在每个节点上支持 K8s 集群运行的服务。 有状态服务集（PetSet） K8s 在 1.3 版本里发布了 Alpha 版的 PetSet 功能。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC 和 RS 主要是控制提供无状态服务的，其所控制的 Pod 的名字是随机设置的，一个 Pod 出故障了就被丢弃掉，在另一个地方重启一个新的 Pod，名字变了、名字和启动在哪儿都不重要，重要的只是 Pod 总数；而 PetSet 是用来控制有状态服务，PetSet 中的每个 Pod 的名字都是事先确定的，不能更改。PetSet 中 Pod 的名字的作用，并不是《千与千寻》的人性原因，而是关联与该 Pod 对应的状态。 对于 RC 和 RS 中的 Pod，一般不挂载存储或者挂载共享存储，保存的是所有 Pod 共享的状态，Pod 像牲畜一样没有分别（这似乎也确实意味着失去了人性特征）；对于 PetSet 中的 Pod，每个 Pod 挂载自己独立的存储，如果一个 Pod 出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来 Pod 的存储继续以它的状态提供服务。 适合于 PetSet 的业务包括数据库服务 MySQL 和 PostgreSQL，集群化管理服务 Zookeeper、etcd 等有状态服务。PetSet 的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用 PetSet，Pod 仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，PetSet 做的只是将确定的 Pod 与确定的存储关联起来保证状态的连续性。PetSet 还只在 Alpha 阶段，后面的设计如何演变，我们还要继续观察。 集群联邦（Federation） K8s 在 1.3 版本里发布了 beta 版的 Federation 功能。在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。K8s 的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足 K8s 的调度和计算存储连接要求。而联合集群服务就是为提供跨 Region 跨服务商 K8s 集群服务而设计的。 每个 K8s Federation 有自己的分布式存储、API Server 和 Controller Manager。用户可以通过 Federation 的 API Server 注册该 Federation 的成员 K8s Cluster。当用户通过 Federation 的 API Server 创建、更改 API 对象时，Federation API Server 会在自己所有注册的子 K8s Cluster 都创建一份对应的 API 对象。在提供业务请求服务时，K8s Federation 会先在自己的各个子 Cluster 之间做负载均衡，而对于发送到某个具体 K8s Cluster 的业务请求，会依照这个 K8s Cluster 独立提供服务时一样的调度模式去做 K8s Cluster 内部的负载均衡。而 Cluster 之间的负载均衡是通过域名服务的负载均衡来实现的。 所有的设计都尽量不影响 K8s Cluster 现有的工作机制，这样对于每个子 K8s 集群来说，并不需要更外层的有一个 K8s Federation，也就是意味着所有现有的 K8s代码和机制不需要因为 Federation 功能有任何变化。 存储卷（Volume） K8s 集群中的存储卷跟 Docker 的存储卷有些类似，只不过 Docker 的存储卷作用范围为一个容器，而 K8s 的存储卷的生命周期和作用范围是一个 Pod。每个 Pod 中声明的存储卷由 Pod 中的所有容器共享。K8s 支持非常多的存储卷类型，特别的，支持多种公有云平台的存储，包括 AWS，Google 和 Azure 云；支持多种分布式存储包括 GlusterFS 和 Ceph；也支持较容易使用的主机本地目录 hostPath 和 NFS。K8s 还支持使用 Persistent Volume Claim 即 PVC 这种逻辑存储，使用这种存储，使得存储的使用者可以忽略后台的实际存储技术（例如 AWS，Google 或 GlusterFS 和 Ceph），而将有关存储实际技术的配置交给存储管理员通过 Persistent Volume 来配置。 持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC） PV 和 PVC 使得 K8s 集群具备了存储的逻辑抽象能力，使得在配置 Pod 的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给 PV 的配置者，即集群的管理者。存储的 PV 和 PVC 的这种关系，跟计算的 Node 和 Pod 的关系是非常类似的；PV 和 Node 是资源的提供者，根据集群的基础设施变化而变化，由 K8s 集群管理员配置；而 PVC 和 Pod 是资源的使用者，根据业务服务的需求变化而变化，由 K8s 集群的使用者即服务的管理员来配置。 节点（Node） K8s 集群中的计算能力由 Node 提供，最初 Node 称为服务节点 Minion，后来改名为 Node。K8s 集群中的 Node 也就等同于 Mesos 集群中的 Slave 节点，是所有 Pod 运行所在的工作主机，可以是物理机也可以是虚拟机。不论是物理机还是虚拟机，工作主机的统一特征是上面要运行 kubelet 管理节点上运行的容器。 密钥对象（Secret） Secret 是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。使用 Secret 的好处是可以避免把敏感信息明文写在配置文件里。在 K8s 集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问 AWS 存储的用户名密码。为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个 Secret 对象，而在配置文件中通过 Secret 对象引用这些敏感信息。这种方式的好处包括：意图明确，避免重复，减少暴漏机会。 用户帐户（User Account）和服务帐户（Service Account） 顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和 K8s 集群中运行的 Pod 提供账户标识。用户帐户和服务帐户的一个区别是作用范围；用户帐户对应的是人的身份，人的身份与服务的 namespace 无关，所以用户账户是跨 namespace 的；而服务帐户对应的是一个运行中程序的身份，与特定 namespace 是相关的。 名字空间（Namespace） 名字空间为 K8s 集群提供虚拟的隔离作用，K8s 集群初始有两个名字空间，分别是默认名字空间 default 和系统名字空间 kube-system，除此以外，管理员可以可以创建新的名字空间满足需要。 RBAC 访问授权 K8s 在 1.3 版本中发布了 alpha 版的基于角色的访问控制（Role-based Access Control，RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control，ABAC），RBAC 主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。在 ABAC 中，K8s 集群中的访问策略只能跟用户直接关联；而在 RBAC 中，访问策略可以跟某个角色关联，具体的用户在跟一个或多个角色相关联。显然，RBAC 像其他新功能一样，每次引入新功能，都会引入新的 API 对象，从而引入新的概念抽象，而这一新的概念抽象一定会使集群服务管理和使用更容易扩展和重用。 总结 从 K8s 的系统架构、技术概念和设计理念，我们可以看到 K8s 系统最核心的两个设计理念：一个是容错性，一个是易扩展性。容错性实际是保证 K8s 系统稳定性和安全性的基础，易扩展性是保证 K8s 对变更友好，可以快速迭代增加新功能的基础。 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/namespace.html":{"url":"basic/namespace.html","title":"kubernetes Namespace","keywords":"","body":"kubernetes Namespace Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services, replication controllers和 deployments 等都是属于某一个 namespace 的（默认是 default），而 node, persistentVolumes 等则不属于任何 namespace。 Namespace 常用来隔离不同的用户，比如 Kubernetes 自带的服务一般运行在 kube-system namespace 中。 注意： namespace 包含两种状态 Active 和 Terminating。在 namespace 删除过程中，namespace 状态被设置成 Terminating。 命名空间名称满足正则表达式 [a-z0-9]([-a-z0-9]*[a-z0-9])?，最大长度为 63 位。 删除一个 namespace 会自动删除所有属于该 namespace 的资源。 default 和 kube-system 命名空间不可删除。 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/node.html":{"url":"basic/node.html","title":"kubernetes Node","keywords":"","body":"kubernetes Node Node 是 Pod 真正运行的主机，可以物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 docker、kubelet 和 kube-proxy 服务。 默认情况下，kubelet 在启动时会向 master 注册自己，并创建 Node 资源。 每个Node都包括以下状态信息 地址(Addresses)：包括 hostname、外网 IP 和内网 IP 条件(Condition)：包括 OutOfDisk、Ready、MemoryPressure 和 DiskPressure 容量(Capacity)：Node 上的可用资源，包括 CPU、内存和 Pod 总数 基本信息(Info)：包括内核版本、容器引擎版本、OS 类型等 例如： $ kubectl describe node 172.20.32.127 Name: 172.20.32.127 Roles: Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/fluentd-ds-ready=true kubernetes.io/hostname=172.20.32.127 Annotations: node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=true Taints: CreationTimestamp: Sun, 17 Jun 2018 20:46:23 +0800 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- OutOfDisk False Fri, 19 Oct 2018 17:49:36 +0800 Sun, 17 Jun 2018 20:48:57 +0800 KubeletHasSufficientDisk kubelet has sufficient disk space available MemoryPressure False Fri, 19 Oct 2018 17:49:36 +0800 Fri, 07 Sep 2018 08:25:56 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 19 Oct 2018 17:49:36 +0800 Fri, 07 Sep 2018 08:25:56 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure Ready True Fri, 19 Oct 2018 17:49:36 +0800 Fri, 07 Sep 2018 08:25:56 +0800 KubeletReady kubelet is posting ready status Addresses: InternalIP: 172.20.32.127 Hostname: 172.20.32.127 Capacity: cpu: 4 memory: 32946920Ki pods: 110 Allocatable: cpu: 4 memory: 32844520Ki pods: 110 System Info: Machine ID: 18d66e3d8e761558137b041965257d8b System UUID: 18D66E3D-8E76-1558-137B-041965257D8B Boot ID: 9a9e4cf7-97ca-4d3e-99aa-39be7a232901 Kernel Version: 3.10.0-862.3.2.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.3.0 Kubelet Version: v1.9.6 Kube-Proxy Version: v1.9.6 PodCIDR: 192.170.11.0/24 ExternalID: 172.20.32.127 Non-terminated Pods: (26 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- kube-system calico-node-xmsj8 250m (6%) 0 (0%) 0 (0%) 0 (0%) kube-system elasticsearch-fluentd-v2.0.2-2b4bj 100m (2%) 300m (7%) 200Mi (0%) 500Mi (1%) kube-system kube-dns-766cb58688-m59dv 260m (6%) 0 (0%) 110Mi (0%) 170Mi (0%) monitoring prometheus-node-exporter-kc27n 0 (0%) 0 (0%) 0 (0%) 0 (0%) st-f1-pack bz-bl-mdm-web.7.0.72-jre7-v7n24 0 (0%) 0 (0%) 0 (0%) 0 (0%) st-f1-pack bz-bl-static-web.7.0.72-jre7-ggb6w 0 (0%) 0 (0%) 0 (0%) 0 (0%) st-f1-pack bz-blf1-common-web.7.0.72-jre7-8t4qp 0 (0%) 0 (0%) 0 (0%) 0 (0%) zabbix zabbix-agent-4mwgz 0 (0%) 0 (0%) 0 (0%) 0 (0%) Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 610m (15%) 300m (7%) 310Mi (0%) 670Mi (2%) Events: Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/service.html":{"url":"basic/service.html","title":"kubernetes Service","keywords":"","body":"kubernetes Service Kubernetes Pod 是平凡的，它们会被创建，也会死掉，并且他们是不可复活的。 Replication Controllers 动态的创建和销毁 Pods(比如规模扩大或者缩小，或者执行动态更新)。每个 pod 都有自己的 IP，这些 IP 也随着时间的变化也不能持续依赖。这样就引发了一个问题：如果一些 Pods（让我们叫它作后台）提供了一些功能供其它的 Pod 使用（让我们叫作前台），在 kubernete 集群中是如何实现让这些前台能够持续的追踪到这些后台的？ 答案是：Service Kubernete Service 是一个定义了一组 Pod 的策略的抽象，我们也有时候叫做宏观服务。这些被服务标记的Pod都是通过 label Selector 决定的。 对于 Kubernete 原生的应用，Kubernete 提供了一个简单的 Endpoints API，这个 Endpoints api 的作用就是当一个服务中的 Pod 发生变化时，Endpoints API 随之变化，对于那些不是原生的程序，Kubernetes 提供了一个基于虚拟 IP 的网桥的服务，这个服务会将请求转发到对应的后台Pod。 每一个节点上都运行了一个 kube-proxy，这个应用监控着 Kubermaster 增加和删除服务，对于服务，kube-proxy 会随机开启一个本机端口，任何发向这个端口的请求都会被转发到一个后台的 Pod 当中，而如何选择是哪一个后台的 Pod 的是基于 SessionAffinity 进行的分配。kube-proxy 会增加 iptables rules 来实现捕捉这个服务的 IP 和端口来并重定向到前面提到的端口。最终的结果就是所有的对于这个服务的请求都会被转发到后台的Pod中。 Service 特性 支持 TCP 和 UDP，但是默认的是 TCP。 可以将一个入端口转发到任何目标端口。 默认情况下 targetPort 的值会和 port 的值相同。 没有选择器的服务 适用场景： 有一个额外的数据库云在生产环境中，但是在测试的时候，希望使用自己的数据库 希望将服务指向其它的服务或者其它命名空间或者其它的云平台上的服务 正在向 kubernete 迁移，并且后台并没有在 Kubernete 中 { “kind”: “Service”, “apiVersion”: “v1″, “metadata”: { “name”: “my-service” }, “spec”: { “ports”: [ { “protocol”: “TCP”, “port”: 80, “targetPort”: 9376 } ] } } { “kind”: “Endpoints”, “apiVersion”: “v1″, “metadata”: { “name”: “my-service” }, “subsets”: [ { “addresses”: [ { “IP”: “1.2.3.4” } ], “ports”: [ { “port”: 80 } ] } ] } 这样的话，这个服务虽然没有 selector，但是却可以正常工作，所有的请求都会被转发到 1.2.3.4：80 Multi-Port Services（多端口服务） 可能很多服务需要开发不止一个端口,为了满足这样的情况，Kubernetes 允许在定义时候指定多个端口，当我们使用多个端口的时候，我们需要指定所有端口的名称，这样 endpoints 才能清楚。 选择自己的 IP 地址 可以在创建服务的时候指定 IP 地址，将 spec.clusterIP 的值设定为我们想要的IP地址即可。选择的 IP 地址必须是一个有效的 IP 地址，并且要在 API server 分配的 IP 范围内，如果这个 IP 地址是不可用的，apiserver 会返回 422http 错误代码来告知是 IP 地址不可用。 服务发现 Kubernetes 支持两种方式的来发现服务 ，环境变量和 DNS。 环境变量 当一个Pod在一个node上运行时，kubelet 会针对运行的服务增加一系列的环境变量，它支持Docker links compatible 和普通环境变量 例如： redis-master 服务暴露了 TCP 6379 端口，并且被分配了 10.0.0.11 IP 地址 那么我们就会有如下的环境变量 REDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 使用环境变量对系统有一个要求：所有的想要被POD访问的服务，必须在POD创建之前创建，否则这个环境变量不会被填充，使用 DNS 则没有这个问题 DNS Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/volume.html":{"url":"basic/volume.html","title":"kubernetes Volume","keywords":"","body":"kubernetes Volume 容器中的磁盘的生命周期是短暂的，这就带来了一系列的问题： 当一个容器损坏之后，kubelet 会重启这个容器，但是文件会丢失-这个容器会是一个全新的状态。 当很多容器在同一 Pod 中运行的时候，很多时候需要数据文件的共享。 Kubernete Volume 解决了这个问题。 一个 Kubernetes volume，拥有明确的生命周期，与所在的 Pod 的生命周期相同。如果这个 Pod 被删除了，那么这些数据也会被删除。 Types of Volumes Kubernete 支持如下类型的 volume： emptyDir hostPath gcePersistentDisk awsElasticBlockStore nfs iscsi glusterfs rbd gitRepo secret persistentVolumeClaim emptyDir 一个 emptyDir 第一次创建是在一个 Pod 被指定到具体 node 的时候，并且会一直存在在 Pod 的生命周期当中，正如它的名字一样，它初始化是一个空的目录，Pod 中的容器都可以读写这个目录，这个目录可以被挂在到各个容器相同或者不相同的的路径下。当一个 Pod 因为任何原因被移除的时候，这些数据会被永久删除。 注意：一个容器崩溃了不会导致数据的丢失，因为容器的崩溃并不移除 Pod。 emptyDir 磁盘的作用： 普通空间，基于磁盘的数据存储 作为从崩溃中恢复的备份点 存储那些那些需要长久保存的数据，例 web 服务中的数据 默认的，emptyDir 磁盘会存储在主机所使用的媒介上，可能是 SSD，或者网络硬盘，这主要取决于你的环境。当然，我们也可以将 emptyDir.medium 的值设置为 Memory 来告诉 Kubernetes 来挂载一个基于内存的目录 tmpfs，因为 tmpfs 速度会比硬盘快得多，但是，当主机重启的时候所有的数据都会丢失。 hostPath 一个 hostPath 类型的磁盘就是挂载了主机的一个文件或者目录，这个功能可能不是那么常用，但是这个功能提供了一个很强大的突破口对于某些应用来说。 例如，如下情况我们就可能需要用到 hostPath： 某些应用需要用到 docker 的内部文件，这个时候只需要挂载本机的 /var/lib/docker 作为 hostPath 在容器中运行 cAdvisor，这个时候挂载 /dev/cgroups 当使用 hostPath 的时候要注意：从模版文件中创建的 pod 可能会因为主机上文件夹目录的不同而导致一些问题。 gcePersistentDisk GCE 谷歌云盘，暂时用不到。 awsElasticBlockStore aws 云盘，暂时用不到。 nfs nfs 使的我们可以挂载已经存在的共享到的我们的 Pod 中，和 emptyDir 不同的是，emptyDir 会被删除当我们的 Pod 被删除的时候，但是 nfs 不会被删除，仅仅是解除挂在状态而已，这就意味着 NFS 能够允许我们提前对数据进行处理，而且这些数据可以在 Pod 之间相互传递。并且，nfs 可以同时被多个 pod挂载并进行读写。 但需要注意 NFS 的网络存储效率比较低。不适合存在大批量读写操作的容器。 iscsi 允许将现有的 iscsi 磁盘挂载到我们的 Pod 中。 glusterfs 允许 Glusterfs 格式的开源磁盘挂载到我们的 Pod 中。 rbd 允许 Rados Block Device 格式的磁盘挂载到我们的Pod中。 gitRepo gitRepo是一个磁盘插件的例子，它挂载了一个空的目录，并且将 git 上的内容 clone 到目录里供 pod 使用。 Secrets Secrets 磁盘是存储敏感信息的磁盘，例如密码之类。我们可以将 secrets 存储到 api 中，使用的时候以文件的形式挂载到 pod 中，而不用连接 api。Secrets 是通过 tmpfs 来支撑的，所以 secrets 永远不会存储到不稳定的地方。 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"install/deployment_architecture.html":{"url":"install/deployment_architecture.html","title":"部署架构","keywords":"","body":"部署架构 参考文档 Kubernetes v1.11.x HA全手动苦工安装教学 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"install/ansible.html":{"url":"install/ansible.html","title":"ansible 方式","keywords":"","body":"ansible 方式 使用 ansible 方式安装 K8s 集群是二进制安装的升级版本，意在将二进制安装的过程封装起来，大大降低搭建集群的复杂度，也便于知识的总结与升华。 目前我主要使用自己维护的 kubeasz 项目搭建公司的 K8s 集群，由于涉及公司的一些信息，暂不考虑开源。 这个项目来源于 github 上的一个优秀的开源项目 kubeasz，需要的朋友可以去 fork 后自己进行定制化处理。 需要有 ansible 和 linux 基础才能玩转此项目。 其他项目也可以参考下： kube-ansible 来自台湾的一名工程师 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"manage/kubectl.html":{"url":"manage/kubectl.html","title":"kubectl","keywords":"","body":"kubectl Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/common_label.html":{"url":"compose/common_label.html","title":"常用标签","keywords":"","body":"常用标签 RestartPoliy 支持三种 Pod 重启策略。 Always：只要退出就重启 OnFailure：失败退出（exit code 不等于0）时重启 Never：只要退出就不再重启 注意，这里的重启是指在Pod所在Node上面本地重启，并不会调度到其他Node上去。 ImagePullPolicy 支持三种镜像拉取策略。 Always：不管镜像是否存在都会进行一次拉取。 Never：不管镜像是否存在都不会进行拉取 IfNotPresent：只有镜像不存在时，才会进行镜像拉取。 注意： 默认为 IfNotPresent，但 :latest 标签的镜像默认为 Always。 拉取镜像时 docker 会进行校验，如果镜像中的 MD5 码没有变，则不会拉取镜像数据。 生产环境中应该尽量避免使用 :latest 标签，而开发环境中可以借助 :latest 标签自动拉取最新的镜像。 资源限制 Kubernetes 通过 cgroups 限制容器的 CPU 和内存等计算资源，包括 requests 和 limits 等： spec.containers[].resources.limits.cpu：CPU上限，可以短暂超过，容器也不会被停止。 spec.containers[].resources.limits.memory：内存上限，不可以超过；如果超过，容器可能会被停止或调度到其他资源充足的机器上。 spec.containers[].resources.requests.cpu：CPU请求，可以超过。 spec.containers[].resources.requests.memory：内存请求，可以超过；但如果超过，容器可能会在Node内存不足时清理。 健康检查 为了确保容器在部署后确实处在正常运行状态，Kubernetes 提供了两种探针，支持 exec、tcp、httpGet 方式，来探测容器的状态： LivenessProbe：探测应用是否处于健康状态，如果不健康则删除重建该容器。 ReadinessProbe：探测应用是否启动完成并且处于正常服务状态，如果不正常则更新容器的状态。 容器生命周期钩子 容器生命周期钩子（Container Lifecycle Hooks）监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数。支持两种钩子： postStart： 容器启动后执行，注意由于是异步执行，它无法保证一定在ENTRYPOINT之后运行。如果失败，容器会被杀死，并根据RestartPolicy决定是否重启 preStop：容器停止前执行，常用于资源清理。如果失败，容器同样也会被杀死 而钩子的回调函数支持两种方式： exec：在容器内执行命令 httpGet：向指定URL发起GET请求 postStart和preStop钩子示例： apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler > /usr/share/message\"] preStop: exec: command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] 指定Node 通过nodeSelector，一个Pod可以指定它所想要运行的Node节点。 首先给Node加上标签： kubectl label nodes disktype=ssd 接着，指定该Pod只想运行在带有 disktype=ssd 标签的 Node 上： apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd 限制网络带宽 可以通过给 Pod 增加 kubernetes.io/ingress-bandwidth 和 kubernetes.io/egress-bandwidth 这两个 annotation 来限制 Pod 的网络带宽。 注意：目前只有 kubenet 网络插件支持限制网络带宽，其他CNI网络插件暂不支持这个功能。 apiVersion: v1 kind: Pod metadata: name: qos annotations: kubernetes.io/ingress-bandwidth: 3M kubernetes.io/egress-bandwidth: 4M spec: containers: - name: iperf3 image: networkstatic/iperf3 command: - iperf3 - -s initContainers Init Container 在所有容器运行之前执行，常用来初始化配置。 capabilities 默认情况下，容器都是以非特权容器的方式运行。比如，不能在容器中创建虚拟网卡、配置虚拟网络。 Kubernetes 提供了修改 Capabilities 的机制，可以按需要给给容器增加或删除。比如下面的配置给容器增加了 CAP_NET_ADMIN 并删除了 CAP_KILL。 apiVersion: v1 kind: Pod metadata: name: hello-world spec: containers: - name: friendly-container image: \"alpine:3.4\" command: [\"/bin/echo\", \"hello\", \"world\"] securityContext: capabilities: add: - NET_ADMIN drop: - KILL Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Deployment.html":{"url":"compose/Deployment.html","title":"Deployment","keywords":"","body":"Deployment Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义(declarative)方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括： 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment 比如一个简单的nginx应用可以定义为： apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 扩容： kubectl scale deployment nginx-deployment --replicas 10 如果集群支持 horizontal pod autoscaling 的话，还可以为 Deployment 设置自动扩展： kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 更新镜像也比较简单： kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 回滚： kubectl rollout undo deployment/nginx-deployment Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Secret.html":{"url":"compose/Secret.html","title":"Secret","keywords":"","body":"Secret Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Statefulset.html":{"url":"compose/Statefulset.html","title":"Statefulset","keywords":"","body":"Statefulset Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/DaemonSet.html":{"url":"compose/DaemonSet.html","title":"DaemonSet","keywords":"","body":"DaemonSet Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/ServiceAccount.html":{"url":"compose/ServiceAccount.html","title":"ServiceAccount","keywords":"","body":"ServiceAccount Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/CronJob.html":{"url":"compose/CronJob.html","title":"CronJob","keywords":"","body":"CronJob Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Job.html":{"url":"compose/Job.html","title":"Job","keywords":"","body":"Job Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/ConfigMap.html":{"url":"compose/ConfigMap.html","title":"ConfigMap","keywords":"","body":"ConfigMap Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Ingress.html":{"url":"compose/Ingress.html","title":"Ingress","keywords":"","body":"Ingress Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"log_manager/efk.html":{"url":"log_manager/efk.html","title":"日志归集(EFK)","keywords":"","body":"日志归集(EFK) ES数据定期删除 删除脚本： #!/bin/bash ################################### #删除早于2天的ES集群的索引 ################################### function delete_indices() { comp_date=`date -d \"2 day ago\" +\"%Y-%m-%d\"` date1=\"$1 00:00:00\" date2=\"$comp_date 00:00:00\" t1=`date -d \"$date1\" +%s` t2=`date -d \"$date2\" +%s` if [ $t1 -le $t2 ]; then echo \"$1时间早于$comp_date，进行索引删除\" #转换一下格式，将类似2017-10-01格式转化为2017.10.01 format_date=`echo $1| sed 's/-/\\./g'` curl -XDELETE http://172.20.32.78:32105/*$format_date fi } curl -XGET http://172.20.32.78:32105/_cat/indices | awk -F\" \" '{print $3}' | awk -F\"-\" '{print $NF}' | egrep \"[0-9]*\\.[0-9]*\\.[0-9]*\" | sort | uniq | sed 's/\\./-/g' | while read LINE do #调用索引删除函数 delete_indices $LINE done 手工处理时区后时间显示不正确？ 存储到ES的数据会有一个字段名为@timestamp，该时间戳和北京时间差了8小时，不需要进行调整，Kibana在展示的时候会自动加上8小时 Kibana登录认证设置 elasticsearch定期删除策略 - 日志分析系统ELK搭建 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"log_manager/real_time_log.html":{"url":"log_manager/real_time_log.html","title":"实时日志处理","keywords":"","body":"实时日志处理 使用 kubectl logs -f 可以非常方便地集中查看集群当中各个 pod 的日志，但是通常需要看日志的是开发人员，而不是运维人员，kubectl 命令对集群管理的权限又很大，因此不可能将 kubectl 的权限开放给开发人员。经过分析，找到了方法，就是使用 rbash 来限制开发人员只能执行自定义的查看日志的命令，而不能进行其他任何操作。 大致步骤如下(需在任意一台安装有 kubectl 命令的集群机器上执行)： 添加一个查看日志的用户 adduser k8sloger passwd k8sloger 使用 rbash 限制用户部分权限 ln -s /bin/bash /bin/rbash bash -c 'echo \"/bin/rbash\" >> /etc/shells' chsh -s /bin/rbash k8sloger 回收日志用户所有的权限 bash -c 'echo \"export PATH=/home/k8sloger/\" >> /home/k8sloger/.bashrc' 赋予日志用户最基本的权限 ln -s /bin/ping /home/k8sloger/ping ln -s /bin/ls /home/k8sloger/ls ln -s /bin/grep /home/k8sloger/grep ln -s /bin/wc /home/k8sloger/wc ln -s /bin/awk /home/k8sloger/awk ln -s /bin/sudo /home/k8sloger/sudo 添加 sudo 权限 visudo # 手工添加以下内容 # k8sloger ALL=NOPASSWD: /root/local/bin/kubectl 添加查看日志的脚本 tee /home/k8sloger/getlog 应用方法 使用 k8sloger 用户 ssh 登陆到服务器(这里建议开发使用 webssh 登陆)，然后执行 getlog 命令根据提示进行操作，即可看到实时日志了。 参考资料 想建一个用户只能执行几条命令 rbash限制用户执行的命令 rbash - 一个受限的Bash Shell用实际示例说明 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"version_record.html":{"url":"version_record.html","title":"版本使用记录","keywords":"","body":"版本使用记录 时间轴 2016.04 docker 容器化研究 2016.05 docker 私仓建立 2016.07 基础组件容器化完毕 2016.09 使用 docker run + 在宿主机上映射war包的方式，实现了第一版的“容器化” 2016.10 Kubernetes 1.4 版本预研 2017.03 Kubernetes 1.4 版本开发集群搭建 2017.05 引入 spring boot 框架，实现应用在 jenkins 上构建应用镜像包，通过 k8s 来发布，实现了第二版部分新应用的容器化 2017.07 Kubernetes 1.4 版本生产集群搭建 2017.09 Kubernetes 1.7 安装部署方式对比研究 2017.11 引入 spring cloud 体系，并改造老工程，实现了第三版真正意义上的应用全容器化 2018.02 使用 kubeasz 项目集成 Kubernetes 1.9 版本的安装部署（升级网络组件 weave 为 calico，集成 prometheus、zabbix 监控工具） 2018.05 使用 kubeasz 项目升级开发 Kubernetes 集群为 1.9.6 高可用集群 2018.06 应用去 dubbo，全部使用 spring cloud 体系 2018.07 使用 kubeasz 项目升级生产 Kubernetes 集群为 1.9.6 高可用集群 2018.09 kubeasz 项目新增 EFK 的部署并在开发集群启用 新版本重要功能 1.10 应用程序编程接口(API)聚合现在在 Kubernetes 1.10 中是稳定的。这使得 Kubernetes 开发人员可以开发自己的自定义API服务器，无需更改核心 Kubernetes 代码库。有了这个特性，Kubernetes 集群管理员可以更放心地在生产环境配置中将这些第三方API添加到集群中。 1.11 管理员现在可以改变一组容器可用的数据存储量，而无需先关闭容器和先卸载现有的存储容量。其结果是减少了停机时间，这将使企业在操作需求发生变化时更容易更新容器集群。 增加了对名为 CoreDNS 的新域名服务器系统的支持。根据发行说明，它具有“比以前DNS服务器更少的移动部件”，以及具有自定义选项可实现更广泛的用例。 1.12 继续关注内部改进与功能完善，旨在进一步提升与 Kubernetes 对接时的稳定性。这一最新版本亦在安全性与 Azure 等关键功能上做出增强。 将带来更强大的安全性、可用性、弹性以及易用性 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"technology_selection.html":{"url":"technology_selection.html","title":"技术选型","keywords":"","body":"技术选型 操作系统选型 CoreOS，一款 Linux 容器发行版 网络组件选型 Docker网络解决方案-Calico部署记录 Kubernetes网络原理及方案 容器网络那些事儿 ingress 选型 8款开源的Kubernetes Ingress Controller/API Gateway推荐 Kubernetes Ingress 对比 Kubernetes高可用负载均衡与集群外服务访问实践 基于Zuul2.0搭建微服务网关以及和NGINX的测试对比 存储选型 Ceph和GFS比较，各有哪些优缺点？ 说实话，这个基本没有可比性～ 虽然 Sage 在最初设计 Ceph 的时候是作为一个分布式存储系统，也就是说其实 CephFS 才是最初的构想和设计(题外音)，但可以看到，后面 Ceph 俨然已经发展为一整套存储解决方案，上层能够提供对象存储(RGW)、块存储(RBD)和CephFS，可以说是一套适合各种场景，非常灵活，非常有可发挥空间的存储解决方案～ 而反观 GFS ，则主要是 Google 为其大数据服务设计开发的底层文件系统，从各种资料中能够看到，其为大数据处理场景做了各种假设、定制和优化，可以说是一套专门针对大数据应用场景的，定制化程度非常高的存储解决方案。 Ceph vs Gluster之开源存储力量的较量 分布式文件系统MFS、Ceph、GlusterFS、Lustre的比较 数据库容器化选型 MySQL到底能不能放到 Docker 里跑？同程旅游竟这么玩 微服务架构 微服务架构最佳实践课堂PPT- 微服务容器化的挑战和解决之道 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-23 17:16:55 "}}