{"./":{"url":"./","title":"我的 kubernetes 学习手册","keywords":"","body":"简介 Kubernetes 是一个开源的，用于管理多个主机上的容器化的应用，Kubernetes 的目标是让部署容器化的应用简单并且高效，Kubernetes 提供了应用部署、规划、更新、维护的一种机制。 Kubernetes 一个核心的特点就是能够自主的管理容器来保证平台中的容器按照用户的期望状态运行。 在 Kubenetes中，所有的容器均在 Pod 中运行，一个 Pod 可以承载一个或者多个相关的容器，同一个 Pod 中的容器会部署在同一个物理机器上并且能够共享资源。一个 Pod 也可以包含 O 个或者多个磁盘卷组（volumes），这些卷组将会以目录的形式提供给一个容器，或者被所有 Pod 中的容器共享。 Kubernetes 提供了服务的抽象，并提供了固定的 IP 地址和 DNS 名称，而这些与一系列 Pod 进行动态关联，所以我们可以关联任何我们想关联的Pod，当一个 Pod 中的容器访问这个地址的时候，这个请求会被转发到本地代理（kube proxy），每台机器上均有一个本地代理，然后被转发到相应的后端容器。Kubernetes 通过一种轮询机制选择相应的后端容器，这些动态的Pod被替换的时候，Kube proxy 时刻追踪着，所以，服务的 IP地址（dns名称），从来不变。 起源 在 Docker 作为高级容器引擎快速发展的同时，Google 也开始将自身在容器技术及集群方面的积累贡献出来。在 Google 内部，容器技术已经应用了很多年，Borg 系统运行管理着成千上万的容器应用，在它的支持下，无论是谷歌搜索、Gmail 还是谷歌地图，可以轻而易举地从庞大的数据中心中获取技术资源来支撑服务运行。 Borg 是集群的管理器，在它的系统中，运行着众多集群，而每个集群可由成千上万的服务器联接组成，Borg 每时每刻都在处理来自众多应用程序所提交的成百上千的 Job，对这些 Job 进行接收、调度、启动、停止、重启和监控。正如 Borg 论文中所说，Borg 提供了 3 大好处： 隐藏资源管理和错误处理，用户仅需要关注应用的开发。 服务高可用、高可靠。 可将负载运行在由成千上万的机器联合而成的集群中。 作为 Google 的竞争技术优势，Borg 理所当然的被视为商业秘密隐藏起来，但当 Tiwtter 的工程师精心打造出属于自己的 Borg 系统（Mesos）时，Google 也审时度势地推出了来源于自身技术理论的新的开源工具。 2014 年 6 月，谷歌云计算专家埃里克·布鲁尔（Eric Brewer）在旧金山的发布会为这款新的开源工具揭牌，它的名字 Kubernetes 在希腊语中意思是船长或领航员，这也恰好与它在容器集群管理中的作用吻合，即作为装载了集装箱（Container）的众多货船的指挥者，负担着全局调度和运行监控的职责。 虽然 Google 推出 Kubernetes 的目的之一是推广其周边的计算引擎（Google Compute Engine）和谷歌应用引擎（Google App Engine）。但 Kubernetes 的出现能让更多的互联网企业可以享受到连接众多计算机成为集群资源池的好处。 Kubernetes 对计算资源进行了更高层次的抽象，通过将容器进行细致的组合，将最终的应用服务交给用户。Kubernetes 在模型建立之初就考虑了容器跨机连接的要求，支持多种网络解决方案，同时在 Service 层次构建集群范围的 SDN 网络。其目的是将服务发现和负载均衡放置到容器可达的范围，这种透明的方式便利了各个服务间的通信，并为微服务架构的实践提供了平台基础。而在 Pod 层次上，作为 Kubernetes 可操作的最小对象，其特征更是对微服务架构的原生支持。 Kubernetes 项目来源于 Borg，可以说是集结了 Borg 设计思想的精华，并且吸收了 Borg 系统中的经验和教训。 Kubernetes 作为容器集群管理工具，于 2015 年 7 月 22 日迭代到 v1.0 并正式对外公布，这意味着这个开源容器编排系统可以正式在生产环境使用。与此同时，谷歌联合 Linux 基金会及其他合作伙伴共同成立了 CNCF 基金会(Cloud Native Computing Foundation)，并将 Kuberentes 作为首个编入 CNCF 管理体系的开源项目，助力容器技术生态的发展进步。Kubernetes 项目凝结了 Google 过去十年间在生产环境的经验和教训，从 Borg 的多任务分配资源块到 Kubernetes 的多副本 Pod，从 Borg 的 Cell 集群管理，到 Kubernetes 设计理念中的联邦集群，在 Docker 等高级引擎带动容器技术兴起和大众化的同时，为容器集群管理提供独了到见解和新思路。 参考资料 服务网格实践手册 Kubernetes Handbook——Kubernetes中文指南/云原生应用架构实践手册 生产级别的容器编排系统 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-12 23:36:52 "},"basic/architecture.html":{"url":"basic/architecture.html","title":"kubernetes 体系结构","keywords":"","body":"kubernetes 体系结构 设计架构 Kubernetes 集群包含有节点代理 kubelet 和 Master 组件(APIs，scheduler，etc)，一切都基于分布式的存储系统。下面这张图是 Kubernetes 的架构图。 在这张系统架构图中，我们把服务分为运行在工作节点上的服务和组成集群级别控制板的服务。 Kubernetes节点有运行应用容器必备的服务，而这些都是受 Master 的控制。 每次个节点上运行 Docker，负责所有具体的镜像下载和容器运行。 核心组件 etcd：保存整个集群的状态； apiserver：提供资源操作的唯一入口，并提供认证、授权、访问控制、API 注册和发现等机制； controller manager：负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； scheduler：负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； kubelet：负责维护容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理； Container runtime：负责镜像管理以及Pod和容器的真正运行（CRI）； kube-proxy：负责为 Service 提供 cluster 内部的服务发现和负载均衡； 附加组件 除了核心组件，还有一些推荐的 Add-ons： kube-dns：负责为整个集群提供 DNS 服务 Ingress Controller：为服务提供外网入口 Heapster：提供资源监控 Dashboard：提供 GUI Federation：提供跨可用区的集群 Fluentd-elasticsearch：提供集群日志采集、存储与查询 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/api.html":{"url":"basic/api.html","title":"kubernetes API 对象","keywords":"","body":"kubernetes API 对象 API 对象是 K8s 集群中的管理操作单元。K8s 集群系统每支持一项新功能，引入一项新技术，一定会新引入对应的 API 对象，支持对该功能的管理操作。 例如副本集 Replica Set 对应的 API 对象是 RS。 每个API对象都有3大类属性：元数据 metadata、规范 spec 和状态 status。 K8s 中所有的配置都是通过 API 对象的 spec 去设置的，也就是用户通过配置系统的理想状态来改变系统，这是 k8s 重要设计理念之一，即所有的操作都是声明式（Declarative）的而不是命令式（Imperative）的。声明式操作在分布式系统中的好处是稳定，不怕丢操作或运行多次，例如设置副本数为 3 的操作运行多次也还是一个结果，而给副本数加 1 的操作就不是声明式的，运行多次结果就错了。 Pod K8s 有很多技术概念，同时对应很多 API 对象，最重要的也是最基础的是微服务 Pod。Pod 是在 K8s 集群中运行部署应用或服务的最小单元，它是可以支持多容器的。Pod 的设计理念是支持多个容器在一个 Pod 中共享网络地址和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。Pod 对多容器的支持是 K8s 最基础的设计理念。比如你运行一个操作系统发行版的软件仓库，一个 Nginx 容器用来发布软件，另一个容器专门用来从源仓库做同步，这两个容器的镜像有可能不是一个团队开发的，但是他们一块儿工作才能提供一个微服务；这种情况下，不同的团队各自开发构建自己的容器镜像，在部署的时候组合成一个微服务对外提供服务。 Pod 是 K8s 集群中所有业务类型的基础，可以看作运行在 K8s 集群中的小机器人，不同类型的业务就需要不同类型的小机器人去执行。目前 K8s 中的业务主要可以分为长期伺服型（long-running）、批处理型（batch）、节点后台支撑型（node-daemon）和有状态应用型（stateful application）；分别对应的小机器人控制器为 Deployment、Job、DaemonSet 和 PetSet。 复制控制器（Replication Controller，RC） RC 是 K8s 集群中最早的保证 Pod 高可用的 API 对象。通过监控运行中的 Pod 来保证集群中运行指定数目的 Pod 副本。指定的数目可以是多个也可以是 1 个；少于指定数目，RC 就会启动运行新的 Pod 副本；多于指定数目，RC 就会杀死多余的 Pod 副本。即使在指定数目为 1 的情况下，通过 RC 运行 Pod 也比直接运行 Pod 更明智，因为 RC 也可以发挥它高可用的能力，保证永远有 1 个 Pod 在运行。RC 是 K8s 较早期的技术概念，只适用于长期伺服型的业务类型，比如提供高可用的 Web 服务。 Replication Controller 只会对那些 RestartPolicy=Always 的 Pod 的生效。 Replication Controller 只支持基于等式的 selector（env=dev 或 environment!=qa） 副本集（Replica Set，RS） RS 是新一代 RC，提供同样的高可用能力，区别主要在于 RS 后来居上，能支持更多种类的匹配模式。副本集对象一般不单独使用，而是作为 Deployment 的理想状态参数使用。 部署（Deployment） 部署表示用户对 K8s 集群的一次更新操作。部署是一个比 RS 应用模式更广的 API 对象，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。滚动升级一个服务，实际是创建一个新的 RS，然后逐渐将新 RS 中副本数增加到理想状态，将旧 RS 中的副本数减小到0的复合操作；这样一个复合操作用一个 RS 是不太好描述的，所以用一个更通用的 Deployment 来描述。以 K8s 的发展方向，未来对所有长期伺服型的的业务的管理，都会通过 Deployment 来管理。 服务（Service） RC、RS 和 Deployment 只是保证了支撑服务的微服务 Pod 的数量，但是没有解决如何访问这些服务的问题。一个 Pod 只是一个运行服务的实例，随时可能在一个节点上停止，在另一个节点以一个新的 IP 启动一个新的 Pod，因此不能以确定的 IP 和端口号提供服务。要稳定地提供服务需要服务发现和负载均衡能力。服务发现完成的工作，是针对客户端访问的服务，找到对应的的后端服务实例。在 K8s 集群中，客户端需要访问的服务就是 Service 对象。每个 Service 会对应一个集群内部有效的虚拟 IP，集群内部通过虚拟 IP 访问一个服务。在 K8s 集群中微服务的负载均衡是由 Kube-proxy 实现的。Kube-proxy 是 K8s 集群内部的负载均衡器。它是一个分布式代理服务器，在 K8s 的每个节点上都有一个；这一设计体现了它的伸缩性优势，需要访问服务的节点越多，提供负载均衡能力的 Kube-proxy 就越多，高可用节点也随之增多。与之相比，我们平时在服务器端做个反向代理做负载均衡，还要进一步解决反向代理的负载均衡和高可用问题。 任务（Job） Job 是 K8s 用来控制批处理型任务的 API 对象。批处理业务与长期伺服业务的主要区别是批处理业务的运行有头有尾，而长期伺服业务在用户不停止的情况下永远运行。Job 管理的 Pod 根据用户的设置把任务成功完成就自动退出了。成功完成的标志根据不同的 spec.completions 策略而不同：单 Pod 型任务有一个 Pod 成功就标志完成；定数成功型任务保证有 N 个任务全部成功；工作队列型任务根据应用确认的全局成功而标志成功。 后台支撑服务集（DaemonSet） 长期伺服型和批处理型服务的核心在业务应用，可能有些节点运行多个同类业务的 Pod，有些节点上又没有这类 Pod 运行；而后台支撑型服务的核心关注点在 K8s 集群中的节点（物理机或虚拟机），要保证每个节点上都有一个此类 Pod 运行。节点可能是所有集群节点也可能是通过 nodeSelector 选定的一些特定节点。典型的后台支撑型服务包括，存储，日志和监控等在每个节点上支持 K8s 集群运行的服务。 有状态服务集（PetSet） K8s 在 1.3 版本里发布了 Alpha 版的 PetSet 功能。在云原生应用的体系里，有下面两组近义词；第一组是无状态（stateless）、牲畜（cattle）、无名（nameless）、可丢弃（disposable）；第二组是有状态（stateful）、宠物（pet）、有名（having name）、不可丢弃（non-disposable）。RC 和 RS 主要是控制提供无状态服务的，其所控制的 Pod 的名字是随机设置的，一个 Pod 出故障了就被丢弃掉，在另一个地方重启一个新的 Pod，名字变了、名字和启动在哪儿都不重要，重要的只是 Pod 总数；而 PetSet 是用来控制有状态服务，PetSet 中的每个 Pod 的名字都是事先确定的，不能更改。PetSet 中 Pod 的名字的作用，并不是《千与千寻》的人性原因，而是关联与该 Pod 对应的状态。 对于 RC 和 RS 中的 Pod，一般不挂载存储或者挂载共享存储，保存的是所有 Pod 共享的状态，Pod 像牲畜一样没有分别（这似乎也确实意味着失去了人性特征）；对于 PetSet 中的 Pod，每个 Pod 挂载自己独立的存储，如果一个 Pod 出现故障，从其他节点启动一个同样名字的Pod，要挂载上原来 Pod 的存储继续以它的状态提供服务。 适合于 PetSet 的业务包括数据库服务 MySQL 和 PostgreSQL，集群化管理服务 Zookeeper、etcd 等有状态服务。PetSet 的另一种典型应用场景是作为一种比普通容器更稳定可靠的模拟虚拟机的机制。传统的虚拟机正是一种有状态的宠物，运维人员需要不断地维护它，容器刚开始流行时，我们用容器来模拟虚拟机使用，所有状态都保存在容器里，而这已被证明是非常不安全、不可靠的。使用 PetSet，Pod 仍然可以通过漂移到不同节点提供高可用，而存储也可以通过外挂的存储来提供高可靠性，PetSet 做的只是将确定的 Pod 与确定的存储关联起来保证状态的连续性。PetSet 还只在 Alpha 阶段，后面的设计如何演变，我们还要继续观察。 集群联邦（Federation） K8s 在 1.3 版本里发布了 beta 版的 Federation 功能。在云计算环境中，服务的作用距离范围从近到远一般可以有：同主机（Host，Node）、跨主机同可用区（Available Zone）、跨可用区同地区（Region）、跨地区同服务商（Cloud Service Provider）、跨云平台。K8s 的设计定位是单一集群在同一个地域内，因为同一个地区的网络性能才能满足 K8s 的调度和计算存储连接要求。而联合集群服务就是为提供跨 Region 跨服务商 K8s 集群服务而设计的。 每个 K8s Federation 有自己的分布式存储、API Server 和 Controller Manager。用户可以通过 Federation 的 API Server 注册该 Federation 的成员 K8s Cluster。当用户通过 Federation 的 API Server 创建、更改 API 对象时，Federation API Server 会在自己所有注册的子 K8s Cluster 都创建一份对应的 API 对象。在提供业务请求服务时，K8s Federation 会先在自己的各个子 Cluster 之间做负载均衡，而对于发送到某个具体 K8s Cluster 的业务请求，会依照这个 K8s Cluster 独立提供服务时一样的调度模式去做 K8s Cluster 内部的负载均衡。而 Cluster 之间的负载均衡是通过域名服务的负载均衡来实现的。 所有的设计都尽量不影响 K8s Cluster 现有的工作机制，这样对于每个子 K8s 集群来说，并不需要更外层的有一个 K8s Federation，也就是意味着所有现有的 K8s代码和机制不需要因为 Federation 功能有任何变化。 存储卷（Volume） K8s 集群中的存储卷跟 Docker 的存储卷有些类似，只不过 Docker 的存储卷作用范围为一个容器，而 K8s 的存储卷的生命周期和作用范围是一个 Pod。每个 Pod 中声明的存储卷由 Pod 中的所有容器共享。K8s 支持非常多的存储卷类型，特别的，支持多种公有云平台的存储，包括 AWS，Google 和 Azure 云；支持多种分布式存储包括 GlusterFS 和 Ceph；也支持较容易使用的主机本地目录 hostPath 和 NFS。K8s 还支持使用 Persistent Volume Claim 即 PVC 这种逻辑存储，使用这种存储，使得存储的使用者可以忽略后台的实际存储技术（例如 AWS，Google 或 GlusterFS 和 Ceph），而将有关存储实际技术的配置交给存储管理员通过 Persistent Volume 来配置。 持久存储卷（Persistent Volume，PV）和持久存储卷声明（Persistent Volume Claim，PVC） PV 和 PVC 使得 K8s 集群具备了存储的逻辑抽象能力，使得在配置 Pod 的逻辑里可以忽略对实际后台存储技术的配置，而把这项配置的工作交给 PV 的配置者，即集群的管理者。存储的 PV 和 PVC 的这种关系，跟计算的 Node 和 Pod 的关系是非常类似的；PV 和 Node 是资源的提供者，根据集群的基础设施变化而变化，由 K8s 集群管理员配置；而 PVC 和 Pod 是资源的使用者，根据业务服务的需求变化而变化，由 K8s 集群的使用者即服务的管理员来配置。 节点（Node） K8s 集群中的计算能力由 Node 提供，最初 Node 称为服务节点 Minion，后来改名为 Node。K8s 集群中的 Node 也就等同于 Mesos 集群中的 Slave 节点，是所有 Pod 运行所在的工作主机，可以是物理机也可以是虚拟机。不论是物理机还是虚拟机，工作主机的统一特征是上面要运行 kubelet 管理节点上运行的容器。 密钥对象（Secret） Secret 是用来保存和传递密码、密钥、认证凭证这些敏感信息的对象。使用 Secret 的好处是可以避免把敏感信息明文写在配置文件里。在 K8s 集群中配置和使用服务不可避免的要用到各种敏感信息实现登录、认证等功能，例如访问 AWS 存储的用户名密码。为了避免将类似的敏感信息明文写在所有需要使用的配置文件中，可以将这些信息存入一个 Secret 对象，而在配置文件中通过 Secret 对象引用这些敏感信息。这种方式的好处包括：意图明确，避免重复，减少暴漏机会。 用户帐户（User Account）和服务帐户（Service Account） 顾名思义，用户帐户为人提供账户标识，而服务账户为计算机进程和 K8s 集群中运行的 Pod 提供账户标识。用户帐户和服务帐户的一个区别是作用范围；用户帐户对应的是人的身份，人的身份与服务的 namespace 无关，所以用户账户是跨 namespace 的；而服务帐户对应的是一个运行中程序的身份，与特定 namespace 是相关的。 名字空间（Namespace） 名字空间为 K8s 集群提供虚拟的隔离作用，K8s 集群初始有两个名字空间，分别是默认名字空间 default 和系统名字空间 kube-system，除此以外，管理员可以可以创建新的名字空间满足需要。 RBAC 访问授权 K8s 在 1.3 版本中发布了 alpha 版的基于角色的访问控制（Role-based Access Control，RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control，ABAC），RBAC 主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。在 ABAC 中，K8s 集群中的访问策略只能跟用户直接关联；而在 RBAC 中，访问策略可以跟某个角色关联，具体的用户在跟一个或多个角色相关联。显然，RBAC 像其他新功能一样，每次引入新功能，都会引入新的 API 对象，从而引入新的概念抽象，而这一新的概念抽象一定会使集群服务管理和使用更容易扩展和重用。 总结 从 K8s 的系统架构、技术概念和设计理念，我们可以看到 K8s 系统最核心的两个设计理念：一个是容错性，一个是易扩展性。容错性实际是保证 K8s 系统稳定性和安全性的基础，易扩展性是保证 K8s 对变更友好，可以快速迭代增加新功能的基础。 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/namespace.html":{"url":"basic/namespace.html","title":"kubernetes Namespace","keywords":"","body":"kubernetes Namespace Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services, replication controllers和 deployments 等都是属于某一个 namespace 的（默认是 default），而 node, persistentVolumes 等则不属于任何 namespace。 Namespace 常用来隔离不同的用户，比如 Kubernetes 自带的服务一般运行在 kube-system namespace 中。 注意： namespace 包含两种状态 Active 和 Terminating。在 namespace 删除过程中，namespace 状态被设置成 Terminating。 命名空间名称满足正则表达式 [a-z0-9]([-a-z0-9]*[a-z0-9])?，最大长度为 63 位。 删除一个 namespace 会自动删除所有属于该 namespace 的资源。 default 和 kube-system 命名空间不可删除。 大多数 Kubernetes 资源（例如pod、services、replication controllers 或其他）都在某些 删除过程中，namespace 中。 低级别资源（如 Node 和 persistentVolumes ）不在任何 namespace 中。 events 是一个例外，它们可能有也可能没有 namespace，具体取决于 events 的对象。 可以通过 Resource Quotas 限制一个 namespace 所可以存取的资源。 管理 namespace 中的资源配额 当用多个团队或者用户共用同一个集群的时候难免会有资源竞争的情况发生，这时候就需要对不同团队或用户的资源使用配额做出限制。 # 查看 Resource Quotas kubectl get resourcequotas -n bst-scm-petrel-dev 管理namespace中的资源配额 参考资料 kubernetes多租户分析 Hypernetes简介——真正多租户的Kubernetes Distro hypernetes stackube Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-11 07:39:42 "},"basic/node.html":{"url":"basic/node.html","title":"kubernetes Node","keywords":"","body":"kubernetes Node Node 是 Pod 真正运行的主机，可以物理机，也可以是虚拟机。为了管理 Pod，每个 Node 节点上至少要运行 docker、kubelet 和 kube-proxy 服务。 默认情况下，kubelet 在启动时会向 master 注册自己，并创建 Node 资源。 每个Node都包括以下状态信息 地址(Addresses)：包括 hostname、外网 IP 和内网 IP 条件(Condition)：包括 OutOfDisk、Ready、MemoryPressure 和 DiskPressure 容量(Capacity)：Node 上的可用资源，包括 CPU、内存和 Pod 总数 基本信息(Info)：包括内核版本、容器引擎版本、OS 类型等 例如： $ kubectl describe node 172.20.32.127 Name: 172.20.32.127 Roles: Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/fluentd-ds-ready=true kubernetes.io/hostname=172.20.32.127 Annotations: node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=true Taints: CreationTimestamp: Sun, 17 Jun 2018 20:46:23 +0800 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- OutOfDisk False Fri, 19 Oct 2018 17:49:36 +0800 Sun, 17 Jun 2018 20:48:57 +0800 KubeletHasSufficientDisk kubelet has sufficient disk space available MemoryPressure False Fri, 19 Oct 2018 17:49:36 +0800 Fri, 07 Sep 2018 08:25:56 +0800 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Fri, 19 Oct 2018 17:49:36 +0800 Fri, 07 Sep 2018 08:25:56 +0800 KubeletHasNoDiskPressure kubelet has no disk pressure Ready True Fri, 19 Oct 2018 17:49:36 +0800 Fri, 07 Sep 2018 08:25:56 +0800 KubeletReady kubelet is posting ready status Addresses: InternalIP: 172.20.32.127 Hostname: 172.20.32.127 Capacity: cpu: 4 memory: 32946920Ki pods: 110 Allocatable: cpu: 4 memory: 32844520Ki pods: 110 System Info: Machine ID: 18d66e3d8e761558137b041965257d8b System UUID: 18D66E3D-8E76-1558-137B-041965257D8B Boot ID: 9a9e4cf7-97ca-4d3e-99aa-39be7a232901 Kernel Version: 3.10.0-862.3.2.el7.x86_64 OS Image: CentOS Linux 7 (Core) Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.3.0 Kubelet Version: v1.9.6 Kube-Proxy Version: v1.9.6 PodCIDR: 192.170.11.0/24 ExternalID: 172.20.32.127 Non-terminated Pods: (26 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits --------- ---- ------------ ---------- --------------- ------------- kube-system calico-node-xmsj8 250m (6%) 0 (0%) 0 (0%) 0 (0%) kube-system elasticsearch-fluentd-v2.0.2-2b4bj 100m (2%) 300m (7%) 200Mi (0%) 500Mi (1%) kube-system kube-dns-766cb58688-m59dv 260m (6%) 0 (0%) 110Mi (0%) 170Mi (0%) monitoring prometheus-node-exporter-kc27n 0 (0%) 0 (0%) 0 (0%) 0 (0%) st-f1-pack bz-bl-mdm-web.7.0.72-jre7-v7n24 0 (0%) 0 (0%) 0 (0%) 0 (0%) st-f1-pack bz-bl-static-web.7.0.72-jre7-ggb6w 0 (0%) 0 (0%) 0 (0%) 0 (0%) st-f1-pack bz-blf1-common-web.7.0.72-jre7-8t4qp 0 (0%) 0 (0%) 0 (0%) 0 (0%) zabbix zabbix-agent-4mwgz 0 (0%) 0 (0%) 0 (0%) 0 (0%) Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) CPU Requests CPU Limits Memory Requests Memory Limits ------------ ---------- --------------- ------------- 610m (15%) 300m (7%) 310Mi (0%) 670Mi (2%) Events: Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/service.html":{"url":"basic/service.html","title":"kubernetes Service","keywords":"","body":"kubernetes Service Kubernetes Pod 是平凡的，它们会被创建，也会死掉，并且他们是不可复活的。 Replication Controllers 动态的创建和销毁 Pods(比如规模扩大或者缩小，或者执行动态更新)。每个 pod 都有自己的 IP，这些 IP 也随着时间的变化也不能持续依赖。这样就引发了一个问题：如果一些 Pods（让我们叫它作后台）提供了一些功能供其它的 Pod 使用（让我们叫作前台），在 kubernete 集群中是如何实现让这些前台能够持续的追踪到这些后台的？ 答案是：Service Kubernete Service 是一个定义了一组 Pod 的策略的抽象，我们也有时候叫做宏观服务。这些被服务标记的Pod都是通过 label Selector 决定的。 对于 Kubernete 原生的应用，Kubernete 提供了一个简单的 Endpoints API，这个 Endpoints api 的作用就是当一个服务中的 Pod 发生变化时，Endpoints API 随之变化，对于那些不是原生的程序，Kubernetes 提供了一个基于虚拟 IP 的网桥的服务，这个服务会将请求转发到对应的后台Pod。 每一个节点上都运行了一个 kube-proxy，这个应用监控着 Kubermaster 增加和删除服务，对于服务，kube-proxy 会随机开启一个本机端口，任何发向这个端口的请求都会被转发到一个后台的 Pod 当中，而如何选择是哪一个后台的 Pod 的是基于 SessionAffinity 进行的分配。kube-proxy 会增加 iptables rules 来实现捕捉这个服务的 IP 和端口来并重定向到前面提到的端口。最终的结果就是所有的对于这个服务的请求都会被转发到后台的Pod中。 Service 特性 支持 TCP 和 UDP，但是默认的是 TCP。 可以将一个入端口转发到任何目标端口。 默认情况下 targetPort 的值会和 port 的值相同。 没有选择器的服务 适用场景： 有一个额外的数据库云在生产环境中，但是在测试的时候，希望使用自己的数据库 希望将服务指向其它的服务或者其它命名空间或者其它的云平台上的服务 正在向 kubernete 迁移，并且后台并没有在 Kubernete 中 { “kind”: “Service”, “apiVersion”: “v1″, “metadata”: { “name”: “my-service” }, “spec”: { “ports”: [ { “protocol”: “TCP”, “port”: 80, “targetPort”: 9376 } ] } } { “kind”: “Endpoints”, “apiVersion”: “v1″, “metadata”: { “name”: “my-service” }, “subsets”: [ { “addresses”: [ { “IP”: “1.2.3.4” } ], “ports”: [ { “port”: 80 } ] } ] } 这样的话，这个服务虽然没有 selector，但是却可以正常工作，所有的请求都会被转发到 1.2.3.4：80 Multi-Port Services（多端口服务） 可能很多服务需要开发不止一个端口,为了满足这样的情况，Kubernetes 允许在定义时候指定多个端口，当我们使用多个端口的时候，我们需要指定所有端口的名称，这样 endpoints 才能清楚。 选择自己的 IP 地址 可以在创建服务的时候指定 IP 地址，将 spec.clusterIP 的值设定为我们想要的IP地址即可。选择的 IP 地址必须是一个有效的 IP 地址，并且要在 API server 分配的 IP 范围内，如果这个 IP 地址是不可用的，apiserver 会返回 422http 错误代码来告知是 IP 地址不可用。 服务发现 Kubernetes 支持两种方式的来发现服务 ，环境变量和 DNS。 环境变量 当一个Pod在一个node上运行时，kubelet 会针对运行的服务增加一系列的环境变量，它支持Docker links compatible 和普通环境变量 例如： redis-master 服务暴露了 TCP 6379 端口，并且被分配了 10.0.0.11 IP 地址 那么我们就会有如下的环境变量 REDIS_MASTER_SERVICE_HOST=10.0.0.11 REDIS_MASTER_SERVICE_PORT=6379 REDIS_MASTER_PORT=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379 REDIS_MASTER_PORT_6379_TCP_PROTO=tcp REDIS_MASTER_PORT_6379_TCP_PORT=6379 REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11 使用环境变量对系统有一个要求：所有的想要被POD访问的服务，必须在POD创建之前创建，否则这个环境变量不会被填充，使用 DNS 则没有这个问题 DNS Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"basic/volume.html":{"url":"basic/volume.html","title":"kubernetes Volume","keywords":"","body":"kubernetes Volume 容器中的磁盘的生命周期是短暂的，这就带来了一系列的问题： 当一个容器损坏之后，kubelet 会重启这个容器，但是文件会丢失-这个容器会是一个全新的状态。 当很多容器在同一 Pod 中运行的时候，很多时候需要数据文件的共享。 Kubernete Volume 解决了这个问题。 一个 Kubernetes volume，拥有明确的生命周期，与所在的 Pod 的生命周期相同。如果这个 Pod 被删除了，那么这些数据也会被删除。 Types of Volumes Kubernete 支持如下类型的 volume： emptyDir hostPath gcePersistentDisk awsElasticBlockStore nfs iscsi glusterfs rbd gitRepo secret persistentVolumeClaim emptyDir 一个 emptyDir 第一次创建是在一个 Pod 被指定到具体 node 的时候，并且会一直存在在 Pod 的生命周期当中，正如它的名字一样，它初始化是一个空的目录，Pod 中的容器都可以读写这个目录，这个目录可以被挂在到各个容器相同或者不相同的的路径下。当一个 Pod 因为任何原因被移除的时候，这些数据会被永久删除。 注意：一个容器崩溃了不会导致数据的丢失，因为容器的崩溃并不移除 Pod。 emptyDir 磁盘的作用： 普通空间，基于磁盘的数据存储 作为从崩溃中恢复的备份点 存储那些那些需要长久保存的数据，例 web 服务中的数据 默认的，emptyDir 磁盘会存储在主机所使用的媒介上，可能是 SSD，或者网络硬盘，这主要取决于你的环境。当然，我们也可以将 emptyDir.medium 的值设置为 Memory 来告诉 Kubernetes 来挂载一个基于内存的目录 tmpfs，因为 tmpfs 速度会比硬盘快得多，但是，当主机重启的时候所有的数据都会丢失。 hostPath 一个 hostPath 类型的磁盘就是挂载了主机的一个文件或者目录，这个功能可能不是那么常用，但是这个功能提供了一个很强大的突破口对于某些应用来说。 例如，如下情况我们就可能需要用到 hostPath： 某些应用需要用到 docker 的内部文件，这个时候只需要挂载本机的 /var/lib/docker 作为 hostPath 在容器中运行 cAdvisor，这个时候挂载 /dev/cgroups 当使用 hostPath 的时候要注意：从模版文件中创建的 pod 可能会因为主机上文件夹目录的不同而导致一些问题。 gcePersistentDisk GCE 谷歌云盘，暂时用不到。 awsElasticBlockStore aws 云盘，暂时用不到。 nfs nfs 使的我们可以挂载已经存在的共享到的我们的 Pod 中，和 emptyDir 不同的是，emptyDir 会被删除当我们的 Pod 被删除的时候，但是 nfs 不会被删除，仅仅是解除挂在状态而已，这就意味着 NFS 能够允许我们提前对数据进行处理，而且这些数据可以在 Pod 之间相互传递。并且，nfs 可以同时被多个 pod挂载并进行读写。 但需要注意 NFS 的网络存储效率比较低。不适合存在大批量读写操作的容器。 iscsi 允许将现有的 iscsi 磁盘挂载到我们的 Pod 中。 glusterfs 允许 Glusterfs 格式的开源磁盘挂载到我们的 Pod 中。 rbd 允许 Rados Block Device 格式的磁盘挂载到我们的Pod中。 gitRepo gitRepo是一个磁盘插件的例子，它挂载了一个空的目录，并且将 git 上的内容 clone 到目录里供 pod 使用。 Secrets Secrets 磁盘是存储敏感信息的磁盘，例如密码之类。我们可以将 secrets 存储到 api 中，使用的时候以文件的形式挂载到 pod 中，而不用连接 api。Secrets 是通过 tmpfs 来支撑的，所以 secrets 永远不会存储到不稳定的地方。 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"network/dns.html":{"url":"network/dns.html","title":"dns","keywords":"","body":"dns Kubernetes之配置与自定义DNS服务 Kubernetes之DNS docker 容器自定义 hosts 网络访问 未容器化之前的现状 上线之前的问题（部署问题） 据了解，开发除了完成自己的开发任务以外，在上线前，需要对自己开发的应用写一个详细的安装说明，来告诉测试、运维如何部署。 不过，即便如此，仍然常常会发生部署失败或者出错的情况，可能有以下原因： 安装说明是基于开发环境编写，且可能写的不够详细，导致配置错误或者步骤遗漏； 部署人员水平的高低不同，特别是测试环境部署人员通常不是专业运维，不了解原理，依葫芦画瓢更容易出错； 运维针对安装说明做了部署上的微调，却没搞清楚应用之间的环境配置依赖问题； 导致的后果（互相指责，推卸责任）： 开发对测试、运维不信任（给了文档还老出问题，不如我自己上了）； 测试、运维责备开发人员文档没写好（把开发抓过来先把环境跑起来再说）； 开发很不爽，反正我的应用没问题，它在开发环境跑的顺溜的很； 增加了各部门人员之间的沟通成本； 最终环境跑起来了，但是完整的文档是没有的； 上线之后的问题（运维问题） 运维噩梦--克隆移植噩梦 老板：这套产品A事业部觉得很好用，B事业部也很想用，XX时间就想要，咱们赶紧给B部门再部署一套吧； 测试：上线这么久了，测试环境与生产环境差异挺大的，能不能帮克隆一套环境到过来做测试； 开发经理：最近开发做了重大调整，为了确保安全，需要在生产部署一套全新的环境来进行灰度发布； 运维经理：这批机器老化了，需要升级，今晚准备做一把迁移吧； 运维：尼玛，昨晚又通宵了。 运维噩梦--资源利用率低 机器资源利用率低 单机多应用无法有效隔离（cpu、内存、硬盘） 开发、测试版本管理复杂 迁移成本高 补丁环境、体验环境 追究原因 应用 环境 脱离 如何解决呢？ 应用如果能带着环境走就好了，就像我们在 windows 下常用的绿色软件一样，放到 u 盘里面到哪里都能用。 带环境安装解决方案 使用虚拟化技术，将应用与环境打包到一起。 主要有2种技术： 虚拟机（virtual machine） Linux容器（Linux container，缩写为LXC） 学习容器的好处 可以快速使用已经容器化的产品，比如 gitlab、mysql、oracle、redis 等，不需要关心部署过程，不需要关心语言环境 可以将重复的工作容器化，比如 lantern、mycat、otter、jmeter 可以轻松拥有各种学习环境 使用容器化以后对各职能的影响 开发： 测试： 运维： 惩罚 走路不穿鞋（扣1分） 洗完手不擦干（扣1分） 用手吃饭（扣1分） 桌上掉饭超过5颗以上（扣1～2分） 上厕所不掀盖子（扣1～2分） 非睡觉时间拿帕子（扣1分） 学习时拖拖拉拉（扣2分） 学习时不听父母指挥（扣2分） 未经允许偷玩电子产品（扣2分） 不尊敬长辈（扣3分） 乱发脾气（扣3分） 用力关门（扣3分） 1小时内累计被扣除8分，或者累计被扣除20分，接受惩罚，惩罚完毕后扣分归零 惩罚方式：自愿接受戒尺惩戒2下或者面壁思过20分钟，如有抵赖行为，惩罚加倍。 奖励 1天内未扣分（奖励5分） 1周内未扣分（奖励25分） 主动做家务（奖励1～3分） 作业完成优良（奖励2～3分） 受到老师的表扬（奖励3～5分） 累计得分 80 动画电影一次 300 自行车一辆 800 假期旅行一次 奖励不能抵消惩罚 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-03 23:17:47 "},"network/calico.html":{"url":"network/calico.html","title":"calico","keywords":"","body":"calico Kubernetes多租户隔离利器-Calico calico 网络结合 k8s networkpolicy 实现租户隔离及部分租户下业务隔离 k8s calico网络原理以及多租户实现设计 容器编排之Kubernetes多租户网络隔离 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-09 13:19:25 "},"install/deployment_architecture.html":{"url":"install/deployment_architecture.html","title":"部署架构","keywords":"","body":"部署架构 参考文档 Kubernetes v1.11.x HA全手动苦工安装教学 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"install/ansible.html":{"url":"install/ansible.html","title":"ansible 方式","keywords":"","body":"ansible 方式 使用 ansible 方式安装 K8s 集群是二进制安装的升级版本，意在将二进制安装的过程封装起来，大大降低搭建集群的复杂度，也便于知识的总结与升华。 目前我主要使用自己维护的 kubeasz 项目搭建公司的 K8s 集群，由于涉及公司的一些信息，暂不考虑开源。 这个项目来源于 github 上的一个优秀的开源项目 kubeasz，需要的朋友可以去 fork 后自己进行定制化处理。 需要有 ansible 和 linux 基础才能玩转此项目。 其他项目也可以参考下： kube-ansible 来自台湾的一名工程师 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"install/rancher.html":{"url":"install/rancher.html","title":"rancher","keywords":"","body":"rancher rancher Kubernetes-基于Rancher进行Kubernetes的离线安装 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-10 00:46:25 "},"manage/kubectl.html":{"url":"manage/kubectl.html","title":"kubectl","keywords":"","body":"kubectl kubectl label nodes 10.0.43.33 k8s.wonhigh.cn/namespace=bst-petrel-st --overwrite kubectl label nodes 10.0.43.9 k8s.wonhigh.cn/namespace=bst-petrel-st --overwrite Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-19 14:17:14 "},"manage/rbac.html":{"url":"manage/rbac.html","title":"rbac","keywords":"","body":"rbac k8s里面有两种用户，一种是User，一种就是service account，User给人用的，service account给进程用的，让进程有相关的权限。 创建 namespace 普通用户 master 操作 # 将用户设置为该命名空间的管理员 kubectl create rolebinding user-bst-scm-petrel-dev-binding --clusterrole=admin --user=user-bst-scm-petrel-dev --namespace=bst-scm-petrel-dev root 操作： adduser k8smanager passwd k8smanager cp /root/local/bin/docker* /usr/local/bin/ cp /root/local/bin/kubectl /usr/local/bin/ usermod -G root k8smanager # 创建 ns kubectl create namespace bst-scm-petrel-dev # 给 node 打上 ns 标签 kubectl label nodes 10.0.43.33 k8s.wonhigh.cn/namespace=bst-scm-petrel-dev kubectl label nodes 10.0.43.9 k8s.wonhigh.cn/namespace=bst-scm-petrel-dev # 复制证书文件 cd /etc/kubernetes/ssl cp admin-csr.json user-bst-scm-petrel-dev-csr.json vim user-bst-scm-petrel-dev-csr.json # 生成证书 /root/local/bin/cfssl gencert \\ -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/ca-config.json \\ -profile=kubernetes user-bst-scm-petrel-dev-csr.json | /root/local/bin/cfssljson -bare user-bst-scm-petrel-dev # 配置集群信息 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.0.43.251:8443 \\ --kubeconfig=user-bst-scm-petrel-dev.kubeconfig chmod +r /etc/kubernetes/ssl/user-bst-scm-petrel-dev* \\cp -f /etc/kubernetes/ssl/user-bst-scm-petrel-dev* /home/k8smanager/ # 查看所有的集群角色 kubectl get clusterrole k8smanager 操作： cd ~ # 配置用户 kubectl config set-credentials user-bst-scm-petrel-dev \\ --client-certificate=user-bst-scm-petrel-dev.pem \\ --embed-certs=true \\ --client-key=user-bst-scm-petrel-dev-key.pem \\ --kubeconfig=user-bst-scm-petrel-dev.kubeconfig # 配置上下文 kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=user-bst-scm-petrel-dev \\ --namespace=bst-scm-petrel-dev \\ --kubeconfig=user-bst-scm-petrel-dev.kubeconfig # 指定上下文 kubectl config use-context kubernetes --kubeconfig=user-bst-scm-petrel-dev.kubeconfig # 覆盖默认的 kubeconfig 文件 \\cp -f ./user-bst-scm-petrel-dev.kubeconfig ~/.kube/config # 查看 token kubectl -n=bst-scm-petrel-dev describe secret $(kubectl -n=bst-scm-petrel-dev get secret | grep user-bst-scm-petrel-dev | awk '{print $1}') 参考资料 创建用户认证授权的kubeconfig文件 kubernetes dashboard访问用户添加权限控制 Managing Service Accounts Creating sample user kubernetes RBAC实战 kubernetes 用户角色访问控制 为 Kubernetes 搭建支持 OpenId Connect 的身份认证系统 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-13 13:38:34 "},"compose/common_label.html":{"url":"compose/common_label.html","title":"常用标签","keywords":"","body":"常用标签 RestartPoliy 支持三种 Pod 重启策略。 Always：只要退出就重启 OnFailure：失败退出（exit code 不等于0）时重启 Never：只要退出就不再重启 注意，这里的重启是指在Pod所在Node上面本地重启，并不会调度到其他Node上去。 ImagePullPolicy 支持三种镜像拉取策略。 Always：不管镜像是否存在都会进行一次拉取。 Never：不管镜像是否存在都不会进行拉取 IfNotPresent：只有镜像不存在时，才会进行镜像拉取。 注意： 默认为 IfNotPresent，但 :latest 标签的镜像默认为 Always。 拉取镜像时 docker 会进行校验，如果镜像中的 MD5 码没有变，则不会拉取镜像数据。 生产环境中应该尽量避免使用 :latest 标签，而开发环境中可以借助 :latest 标签自动拉取最新的镜像。 资源限制 Kubernetes 通过 cgroups 限制容器的 CPU 和内存等计算资源，包括 requests 和 limits 等： spec.containers[].resources.limits.cpu：CPU上限，可以短暂超过，容器也不会被停止。 spec.containers[].resources.limits.memory：内存上限，不可以超过；如果超过，容器可能会被停止或调度到其他资源充足的机器上。 spec.containers[].resources.requests.cpu：CPU请求，可以超过。 spec.containers[].resources.requests.memory：内存请求，可以超过；但如果超过，容器可能会在Node内存不足时清理。 健康检查 为了确保容器在部署后确实处在正常运行状态，Kubernetes 提供了两种探针，支持 exec、tcp、httpGet 方式，来探测容器的状态： LivenessProbe：探测应用是否处于健康状态，如果不健康则删除重建该容器。 ReadinessProbe：探测应用是否启动完成并且处于正常服务状态，如果不正常则更新容器的状态。 容器生命周期钩子 容器生命周期钩子（Container Lifecycle Hooks）监听容器生命周期的特定事件，并在事件发生时执行已注册的回调函数。支持两种钩子： postStart： 容器启动后执行，注意由于是异步执行，它无法保证一定在ENTRYPOINT之后运行。如果失败，容器会被杀死，并根据RestartPolicy决定是否重启 preStop：容器停止前执行，常用于资源清理。如果失败，容器同样也会被杀死 而钩子的回调函数支持两种方式： exec：在容器内执行命令 httpGet：向指定URL发起GET请求 postStart和preStop钩子示例： apiVersion: v1 kind: Pod metadata: name: lifecycle-demo spec: containers: - name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\", \"-c\", \"echo Hello from the postStart handler > /usr/share/message\"] preStop: exec: command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] 指定Node 通过nodeSelector，一个Pod可以指定它所想要运行的Node节点。 首先给Node加上标签： kubectl label nodes disktype=ssd 接着，指定该Pod只想运行在带有 disktype=ssd 标签的 Node 上： apiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent nodeSelector: disktype: ssd 限制网络带宽 可以通过给 Pod 增加 kubernetes.io/ingress-bandwidth 和 kubernetes.io/egress-bandwidth 这两个 annotation 来限制 Pod 的网络带宽。 注意：目前只有 kubenet 网络插件支持限制网络带宽，其他CNI网络插件暂不支持这个功能。 apiVersion: v1 kind: Pod metadata: name: qos annotations: kubernetes.io/ingress-bandwidth: 3M kubernetes.io/egress-bandwidth: 4M spec: containers: - name: iperf3 image: networkstatic/iperf3 command: - iperf3 - -s initContainers Init Container 在所有容器运行之前执行，常用来初始化配置。 capabilities 默认情况下，容器都是以非特权容器的方式运行。比如，不能在容器中创建虚拟网卡、配置虚拟网络。 Kubernetes 提供了修改 Capabilities 的机制，可以按需要给给容器增加或删除。比如下面的配置给容器增加了 CAP_NET_ADMIN 并删除了 CAP_KILL。 apiVersion: v1 kind: Pod metadata: name: hello-world spec: containers: - name: friendly-container image: \"alpine:3.4\" command: [\"/bin/echo\", \"hello\", \"world\"] securityContext: capabilities: add: - NET_ADMIN drop: - KILL Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Deployment.html":{"url":"compose/Deployment.html","title":"Deployment","keywords":"","body":"Deployment Deployment 为 Pod 和 ReplicaSet 提供了一个声明式定义(declarative)方法，用来替代以前的 ReplicationController 来方便的管理应用。典型的应用场景包括： 定义 Deployment 来创建 Pod 和 ReplicaSet 滚动升级和回滚应用 扩容和缩容 暂停和继续 Deployment 比如一个简单的nginx应用可以定义为： apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 3 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 扩容： kubectl scale deployment nginx-deployment --replicas 10 如果集群支持 horizontal pod autoscaling 的话，还可以为 Deployment 设置自动扩展： kubectl autoscale deployment nginx-deployment --min=10 --max=15 --cpu-percent=80 更新镜像也比较简单： kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1 回滚： kubectl rollout undo deployment/nginx-deployment Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Secret.html":{"url":"compose/Secret.html","title":"Secret","keywords":"","body":"Secret Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Statefulset.html":{"url":"compose/Statefulset.html","title":"Statefulset","keywords":"","body":"Statefulset Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/DaemonSet.html":{"url":"compose/DaemonSet.html","title":"DaemonSet","keywords":"","body":"DaemonSet Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/ServiceAccount.html":{"url":"compose/ServiceAccount.html","title":"ServiceAccount","keywords":"","body":"ServiceAccount Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/CronJob.html":{"url":"compose/CronJob.html","title":"CronJob","keywords":"","body":"CronJob Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Job.html":{"url":"compose/Job.html","title":"Job","keywords":"","body":"Job Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/ConfigMap.html":{"url":"compose/ConfigMap.html","title":"ConfigMap","keywords":"","body":"ConfigMap Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"compose/Ingress.html":{"url":"compose/Ingress.html","title":"Ingress","keywords":"","body":"Ingress Træfɪk Træfɪk官网 入门教程，有点晦涩难懂，网站不错。 入门试验： # docker-compose.yml traefik: image: traefik command: --web --docker --docker.domain=docker.localhost --logLevel=DEBUG ports: - \"80:80\" - \"8080:8080\" - \"443:443\" volumes: - /var/run/docker.sock:/var/run/docker.sock - /dev/null:/traefik.toml Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-25 15:36:10 "},"registry/harbor.html":{"url":"registry/harbor.html","title":"harbor","keywords":"","body":"harbor Harbor 是一个用于存储和分发 Docker 镜像的企业级 Registry 服务器，通过添加一些企业必需的功能特性，例如安全、标识和管理等，扩展了开源 Docker Distribution。作为一个企业级私有 Registry 服务器，Harbor 提供了更好的性能和安全。提升用户使用 Registry 构建和运行环境传输镜像的效率。Harbor 支持安装在多个 Registry 节点的镜像资源复制，镜像全部保存在私有 Registry 中， 确保数据和知识产权在公司内部网络中管控。另外，Harbor 也提供了高级的安全特性，诸如用户管理，访问控制和活动审计等。 Harbor 官网 Harbor release 用户指南 Installation & Configuration Guide Harbor on Kubernetes with Harbor chart Configuring Harbor with HTTPS Access 特性 云原生仓库：支持容器镜像和 Helm charts 基于角色的访问控制：用户与Docker镜像仓库通过“项目”进行组织管理，一个用户可以对多个镜像仓库在同一命名空间（project）里有不同的权限。 基于策略的镜像复制：可以基于具有多个过滤器的策略在多个仓库实例之间复制（同步）镜像。如果遇到任何错误，Harbor 将自动重试进行复制。非常适合负载平衡，高可用性，多数据中心，混合和多云场景。 漏洞扫描：Harbor 定期扫描镜像并警告用户漏洞。 图形化用户界面：用户可以通过浏览器来浏览，检索当前Docker镜像仓库，管理项目和命名空间。 LDAP/AD 支持：Harbor可以集成企业内部已有的AD/LDAP，用于鉴权认证管理。 国际化：已拥有英文、中文、德文、日文和俄文的本地化版本。更多的语言将会添加进来。 镜像删除和垃圾收集：可以删除镜像，并可以回收它们的空间。 RESTful API：适用于大多数管理操作的 RESTful API，易于与外部系统集成。 易于部署：提供在线和离线两种安装工具， 也可以安装到vSphere平台(OVA方式)虚拟设备。 安装步骤 cd /tmp wget http://10.0.43.24:8066/harbor-offline-installer-v1.6.1.tgz tar zxvf harbor-offline-installer-v1.6.1.tgz cd harbor # 修改配置 # 启动 harbor sh /data/harbor/install.sh --with-clair --with-chartmuseum docker-compose -f ./docker-compose.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml down -v docker-compose -f ./docker-compose.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml up -d docker-compose -f ./docker-compose.yml -f ./docker-compose.clair.yml -f ./docker-compose.chartmuseum.yml up -d registry-web 权限管理 Harbor基于角色的访问控制，与 project 关联的角色简单地分为 Guest/Developer/Admin 三类，角色/project/镜像三者之间进行关联，不同角色的权限不同： 角色 权限说明 Guest 对于指定项目拥有只读权限 Developer 对于指定项目拥有读写权限 ProjectAdmin 除了读写权限，同时拥有用户管理/镜像扫描等管理权限 镜像回收 风险比较高，必须先备份数据，防止出现意外情况。 而且需要将仓库设置为只读，或者临时下线。 cd /data/harbor docker-compose stop # The above option \"--dry-run\" will print the progress without removing any data docker run -it --name gc --rm --volumes-from registry goharbor/registry:2.6.2-photon garbage-collect --dry-run /etc/registry/config.yml docker run -it --name gc --rm --volumes-from registry goharbor/registry:2.6.2-photon garbage-collect /etc/registry/config.yml docker-compose start 实测结论： 在界面软删除一个镜像，后端执行GC后，镜像空间可以回收，但是镜像的层并没有被删除，因此再次 push 会告知 Layer already exists 由此可见，不能随意地去删除一个镜像，除非确定这个镜像所有的层都不会再用了(一般不好确定，因此不是很占空间的镜像就别删了) 实际应用中，应该将最基础的镜像 push 到仓库中，永不删除，而代码构建的镜像都是基于基础镜像，在删除时只会删除代码构建的那一层，从而可以确保基础镜像层安全，同时要删除的也就是这些业务代码构建的镜像 尽量不要删除所有镜像，至少保留最新的一个版本 在删除镜像前要做好数据备份，镜像删除是一个容易出错的事情 可以考虑使用镜像同步产生新的镜像仓库，将基础镜像同步过去，然后彻底抛弃老仓库 可以考虑使用镜像同步做基础镜像备份 Harbor API 自己写脚本扩展功能时非常重要： # 查看镜像是否在存在于 harbor 仓库 curl -u \"scm:n7izpoc6N2\" -X GET -H \"Content-Type: application/json\" \"http://hub.wonhigh.cn/api/repositories/petrel%2Fpetrel-register-center/tags/1.0.0-SNAPSHOT\" HARBOR 仓库 API功能接口 Harbor REST API说明 问题 Q：管理员登录后没有管理员的操作权限？ A：清除浏览器缓存。 Q：仓库同步报错：hub.wonhigh.cn: no such host A：需要处理 docker-compose.yml 文件，增加对 jobservice 服务的 extra_hosts 参数。 参考资料 VMware Harbor 学习 安装harbor1.6 企业级镜像仓库 Harbor容器镜像安全漏洞扫描详述和视频 探索Harbor镜像仓库新的管理功能和界面 Harbor最新资讯 Harbor实现容器镜像仓库的管理和运维 Harbor基于角色的权限管理 Harbor镜像删除空间回收 删除Docker Registry里的镜像怎么那么难 Harbor Registry Garbage Collect(垃圾回收) docker-maven-plugin 完全免 Dockerfile 文件 Docker镜像仓库Harbor主从镜像同步 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-20 15:45:27 "},"log_manager/efk.html":{"url":"log_manager/efk.html","title":"日志归集(EFK)","keywords":"","body":"日志归集(EFK) ES数据定期删除 删除脚本： #!/bin/bash ################################### #删除早于2天的ES集群的索引 ################################### function delete_indices() { comp_date=`date -d \"2 day ago\" +\"%Y-%m-%d\"` date1=\"$1 00:00:00\" date2=\"$comp_date 00:00:00\" t1=`date -d \"$date1\" +%s` t2=`date -d \"$date2\" +%s` if [ $t1 -le $t2 ]; then echo \"$1时间早于$comp_date，进行索引删除\" #转换一下格式，将类似2017-10-01格式转化为2017.10.01 format_date=`echo $1| sed 's/-/\\./g'` curl -XDELETE http://172.20.32.78:32105/*$format_date fi } curl -XGET http://172.20.32.78:32105/_cat/indices | awk -F\" \" '{print $3}' | awk -F\"-\" '{print $NF}' | egrep \"[0-9]*\\.[0-9]*\\.[0-9]*\" | sort | uniq | sed 's/\\./-/g' | while read LINE do #调用索引删除函数 delete_indices $LINE done 手工处理时区后时间显示不正确？ 存储到ES的数据会有一个字段名为@timestamp，该时间戳和北京时间差了8小时，不需要进行调整，Kibana在展示的时候会自动加上8小时 Kibana登录认证设置 elasticsearch定期删除策略 - 日志分析系统ELK搭建 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"log_manager/real_time_log.html":{"url":"log_manager/real_time_log.html","title":"实时日志处理","keywords":"","body":"实时日志处理 使用 kubectl logs -f 可以非常方便地集中查看集群当中各个 pod 的日志，但是通常需要看日志的是开发人员，而不是运维人员，kubectl 命令对集群管理的权限又很大，因此不可能将 kubectl 的权限开放给开发人员。经过分析，找到了方法，就是使用 rbash 来限制开发人员只能执行自定义的查看日志的命令，而不能进行其他任何操作。 大致步骤如下(需在任意一台安装有 kubectl 命令的集群机器上执行)： 添加一个查看日志的用户 adduser k8sloger passwd k8sloger 使用 rbash 限制用户部分权限 ln -s /bin/bash /bin/rbash bash -c 'echo \"/bin/rbash\" >> /etc/shells' chsh -s /bin/rbash k8sloger 回收日志用户所有的权限 bash -c 'echo \"export PATH=/home/k8sloger/\" >> /home/k8sloger/.bashrc' 赋予日志用户最基本的权限 ln -s /bin/ping /home/k8sloger/ping ln -s /bin/ls /home/k8sloger/ls ln -s /bin/grep /home/k8sloger/grep ln -s /bin/wc /home/k8sloger/wc ln -s /bin/awk /home/k8sloger/awk ln -s /bin/sudo /home/k8sloger/sudo 添加 sudo 权限 visudo # 手工添加以下内容 # k8sloger ALL=NOPASSWD: /root/local/bin/kubectl 添加查看日志的脚本 tee /home/k8sloger/getlog 应用方法 使用 k8sloger 用户 ssh 登陆到服务器(这里建议开发使用 webssh 登陆)，然后执行 getlog 命令根据提示进行操作，即可看到实时日志了。 参考资料 想建一个用户只能执行几条命令 rbash限制用户执行的命令 rbash - 一个受限的Bash Shell用实际示例说明 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-22 23:10:09 "},"temp/helm.html":{"url":"temp/helm.html","title":"helm学习","keywords":"","body":"helm学习 官网 官方帮助文档 安装步骤 安装 helm 客户端 wget http://10.0.43.24:8066/helm/helm-v2.11.0-linux-amd64.tar.gz tar -zxvf helm-v2.11.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm helm help # 添加自动补全 source 安装 helm 服务端 tiller helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.11.0 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts # 为 Tiller 设置帐号 kubectl create serviceaccount --namespace kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' # 验证 Tiller 是否安装成功 kubectl get deploy --namespace kube-system tiller-deploy --output yaml|grep serviceAccount kubectl -n kube-system get pods|grep tiller helm version 卸载 Helm 服务器端 Tiller helm reset --help helm reset helm reset -f # 移除 helm init 创建的目录等数据 helm reset --remove-helm-home # 创建示例 mychart helm create mychart # 检查依赖和模板配置是否正确 helm lint mychart # 将应用打包 helm package mychart --debug # helm serve 命令启动一个 Repository Server， # 该 Server 缺省使用 $HOME/.helm/repository/local 目录作为 Chart 存储，并在 8879 端口上提供服务。 helm serve & # 查看 helm Repository 信息 helm repo list # 将本地 Repository 加入 Helm 的 Repo 列表 helm repo add local http://127.0.0.1:8879 # 查找包 helm search mychart # 部署验证 helm install --dry-run --debug local/mychart --name mike-test # 部署 helm install local/mychart --name mike-test # 列出的所有已部署的 Release 以及其对应的 Chart helm list helm list -a --namespace=tsc # 查询一个特定的 Release 的状态 helm status mike-test 与 harbor 仓库集成 # 安装 push 插件（可以因为网络的原因安装失败，多试几次） helm plugin remove push helm plugin install https://github.com/chartmuseum/helm-push # 添加仓库 helm repo add --username scm --password n7izpoc6N2 repo-petrel http://hub.wonhigh.cn/chartrepo/petrel # 推送 chart 到仓库 helm push --username scm --password n7izpoc6N2 mychart-0.1.0.tgz repo-petrel # 更新仓库 helm repo update # 查找 chart helm search mychart # 部署验证 helm install --dry-run --debug --username scm --password n7izpoc6N2 --version 0.1.0 repo-petrel/mychart --name mike-test # 安装 chart helm install --username scm --password n7izpoc6N2 --version 0.1.0 repo-petrel/mychart --name mike-test # 升级版本 helm upgrade mike-test repo-petrel/mychart helm upgrade mike-test --version 0.1.0 repo-petrel/mychart # 查看历史 helm history mike-test # 回滚版本 helm rollback mike-test 1 # 删除应用 helm delete mike-test # 查看应用状态 helm ls -a mike-test helm ls --deleted # 移除指定 Release 所有相关的 Kubernetes 资源和 Release 的历史记录 helm delete --purge mike-test Helm 入门指南 Kubernetes之Helm包管理 Kubernetes 应用管理工具 Helm 使用指南 harbor-manage-helm-charts Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-16 23:59:08 "},"version_record.html":{"url":"version_record.html","title":"版本使用记录","keywords":"","body":"版本使用记录 时间轴 2016.04 docker 容器化研究 2016.05 docker 私仓建立 2016.07 基础组件容器化完毕 2016.09 使用 docker run + 在宿主机上映射war包的方式，实现了第一版的“容器化” 2016.10 Kubernetes 1.4 版本预研 2017.03 Kubernetes 1.4 版本开发集群搭建 2017.05 引入 spring boot 框架，实现应用在 jenkins 上构建应用镜像包，通过 k8s 来发布，实现了第二版部分新应用的容器化 2017.07 Kubernetes 1.4 版本生产集群搭建 2017.09 Kubernetes 1.7 安装部署方式对比研究 2017.11 引入 spring cloud 体系，并改造老工程，实现了第三版真正意义上的应用全容器化 2018.02 使用 kubeasz 项目集成 Kubernetes 1.9 版本的安装部署（升级网络组件 weave 为 calico，集成 prometheus、zabbix 监控工具） 2018.05 使用 kubeasz 项目升级开发 Kubernetes 集群为 1.9.6 高可用集群 2018.06 应用去 dubbo，全部使用 spring cloud 体系 2018.07 使用 kubeasz 项目升级生产 Kubernetes 集群为 1.9.6 高可用集群 2018.09 kubeasz 项目新增 EFK 的部署并在开发集群启用 2018.11.08 洪大师以非技术角度给大家分享公司为什么迫切需要使用k8s 工作历程 2018.10 公司内部大力推进容器化进程，以 scm 项目作为新的试点，搭建新环境 scm 开发环境容器化推进进度 镜像仓库构建（完成） k8s 集群搭建（完成）【目前为3主5从】 k8s 集群验证（完成）【测试应用正常运行、容器间网络连通正常、集群资源运行正常】 k8s 基础组件 - dashboard 安装（完成） 容器化发布基础组件：文件下载服务、镜像仓库web查看工具、jenkins（完成） k8s 应用部署组件安装（完成） petrel 基础组件在 jenkins 构建并连接 k8s 应用部署组件自动发布（完成） 初始化 k8s-easy 项目，并上传到公司到 gitlab 服务器（完成） petrel 基础工程在 k8s 环境运行联调（完成90%） 镜像仓库切换 harbor 仓库（完成60%） 新版本重要功能 1.10 应用程序编程接口(API)聚合现在在 Kubernetes 1.10 中是稳定的。这使得 Kubernetes 开发人员可以开发自己的自定义API服务器，无需更改核心 Kubernetes 代码库。有了这个特性，Kubernetes 集群管理员可以更放心地在生产环境配置中将这些第三方API添加到集群中。 1.11 管理员现在可以改变一组容器可用的数据存储量，而无需先关闭容器和先卸载现有的存储容量。其结果是减少了停机时间，这将使企业在操作需求发生变化时更容易更新容器集群。 增加了对名为 CoreDNS 的新域名服务器系统的支持。根据发行说明，它具有“比以前DNS服务器更少的移动部件”，以及具有自定义选项可实现更广泛的用例。 1.12 继续关注内部改进与功能完善，旨在进一步提升与 Kubernetes 对接时的稳定性。这一最新版本亦在安全性与 Azure 等关键功能上做出增强。 将带来更强大的安全性、可用性、弹性以及易用性 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-11-09 11:59:44 "},"technology_selection.html":{"url":"technology_selection.html","title":"技术选型","keywords":"","body":"技术选型 操作系统选型 CoreOS，一款 Linux 容器发行版 网络组件选型 Docker网络解决方案-Calico部署记录 Kubernetes网络原理及方案 容器网络那些事儿 ingress 选型 8款开源的Kubernetes Ingress Controller/API Gateway推荐 Kubernetes Ingress 对比 Kubernetes高可用负载均衡与集群外服务访问实践 基于Zuul2.0搭建微服务网关以及和NGINX的测试对比 存储选型 Ceph和GFS比较，各有哪些优缺点？ 说实话，这个基本没有可比性～ 虽然 Sage 在最初设计 Ceph 的时候是作为一个分布式存储系统，也就是说其实 CephFS 才是最初的构想和设计(题外音)，但可以看到，后面 Ceph 俨然已经发展为一整套存储解决方案，上层能够提供对象存储(RGW)、块存储(RBD)和CephFS，可以说是一套适合各种场景，非常灵活，非常有可发挥空间的存储解决方案～ 而反观 GFS ，则主要是 Google 为其大数据服务设计开发的底层文件系统，从各种资料中能够看到，其为大数据处理场景做了各种假设、定制和优化，可以说是一套专门针对大数据应用场景的，定制化程度非常高的存储解决方案。 Ceph vs Gluster之开源存储力量的较量 分布式文件系统MFS、Ceph、GlusterFS、Lustre的比较 数据库容器化选型 MySQL到底能不能放到 Docker 里跑？同程旅游竟这么玩 微服务架构 微服务架构最佳实践课堂PPT- 微服务容器化的挑战和解决之道 Kubernetes和OpenStack到底是什么关系？先搞清楚，再系列学习 Kubernetes 面向应用层，变革的是业务架构，而 OpenStack 面向资源层，改变的是资源供给模式。使用容器且集群规模不大，直接用 Kubenetes 就可以；集群规模大，不管应用是否只是跑在容器中，都是 OpenStack + Kubernetes 更好。 OpenStack + Kubernetes 是各取所长，并不只是因为惯性，而是对于多租户需求来说，Container（容器）的隔离性还需要加强，需要加一层 VM（虚拟机） 来弥补，而 OpenStack 是很好的方案。不过，VM + Container 的模式，必然有性能的损耗，所以 OpenStack 基金会也推出一个项目叫 Kata Containers，希望减少虚拟化的开销，兼顾容器的性能和隔离性。 永恒的只有变化，未来的业务都会运行在云上，容器是走向 DevOps、Cloud Native（云原生）的标准工具，已经开始走向平凡，而 Kubernetes 的编排能力，让容器能够落地到业务应用中，所以我们看到 Docker、Mesos、OpenStack 以及很多公有云、私有云服务商，都在支持 Kubernetes，大家都加入了 CNCF（云原生计算基金会）。 总结起来，OpenStack 是兼容传统的架构，而 Kubernetes 是面向未来的架构。 最后，计算开源云这几年发展很快，从这个问题提出到现在，社区又有了很多变化。所以要修正一个观点：Kubernetes 支持的容器运行时不仅仅是 Docker，也包括 Rkt，当然 Docker 更加流行。 简单的说，kubernetes是管理container的工具，openstack是管理VM的工具。 container可以运行在物理机上，也可以运行在VM上。所以kubernetes不是需要openstack的支持。但对于云计算来说，很多IasS都通过openstack来管理虚拟机。然后用户可以在这些虚拟机上运行docker，可以通过kubernetes进行管理。 不过kubernetes虽然是开源的，但它毕竟是为GCE服务的，Google其实并没有多少动力去支持其他平台的。 京东从OpenStack改用Kubernetes的始末 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-31 10:31:41 "},"qa.html":{"url":"qa.html","title":"问题记录","keywords":"","body":"问题记录 别人分享的问题 Q：使用nfs有存在性能瓶颈或单点故障的问题嘛?如何解决，不如对于持久化要求高的Redis 应该采用哪种存储？ A：具体看你的规模数量，测试、开发环境，单节点nfs毫无压力,数据是先写到缓存内存，速度很快，非常稳写，我文章中的说的内核注意BUG，没必要做高可用，公有云有nas服务不必担心，自建机房可以用drbd Keepalived vip。 Q：为什么网络没有使用traefik,spring cloud的先关组件是怎么部署？是用yaml文件还是使用helm方式？ A：考虑到traefik性能没有nginx好，所以用nginx, ymal是自己写的模板生成，没有用helm,我们正在调研，eureka可以单独定制多个yml互相注册。与外部服务通过打通网络了直通，通过svc对接。 Q：用ceph的方案是ceph provision的方式还是csi plugins的方式，csi plugins好像在1.12.0以上版本才较好地支持，用ceph csi plugins的方式 有遇到性能或可靠性的问题嘛？如何解决？ A：我们现在用的是provision方式，没用csi plugins。 Q：目前有状态容器 statefulset 使用持久化的存储方案，nfs和ceph的方案 有上生产环境嘛？上生产环境的应用是Redis或者MySQL？生产上是否有遇到？ A：生产redis nfs是没问题，生产我们用的公有云，直接用的厂商的存储，ceph只用在测试、开发环境。 Q：用你说的日志方案，你们的集群规模以及服务规模大概是多大，每天可以产生多少日志呢？ A：每天日志1-2TB左右，es集群SSD可以随时扩容。 Q：脚本是通过什么语言开发的 ，能否共享一个例子 A：用shell脚本可以很容易完成，无非是awk sed 一些逻辑，最主要是要有思路，需要例子可以单独联系我，我可以指导。 Q：请问下所有环境都在一个集群，压测怎么办？ A：压测只是对应用产生压力，你可以把需要压测的应用调度到不同的节点NodeSelector隔离运行。 Q：对于局域网微信回调是如何做，没有公网ip A：打通网络之后，设置wifi 指向dns为k8s-dns ，service直接互通。 Q：eureka注册时服务ip用的什么 A：k8s集群内会用的podip去注册。 Q：有状态应用的场景，使用容器部署与传统部署有啥区别？容器部署是否存在一些坑？ A：有状态容器创建后，尽量少动，少迁移，遇到过卡住，容器无法迁移或删除，重要的mysql之类的建议放外部运行。 Q：所有的环境都是跑在一个集群上面吗？ A：当然不是，我们有很多套k8s集群，跨公有云不同平台多个集群。 Q：我想请问一下新版本kubernetes可以兼容旧版本的kubernetes内容吗，可以在哪里查到详情吗? A：k8s 官网，注意yaml api版本，新的一般兼容旧的api. Q：你们在用Eureka做注册中心的时候 服务注册具体服务名称这么写的，得加上Eureka所在的namespace吧 A：服务注册用jar包名就可以，不需要namespace名。 别人的分享经验 公司原有运维系统缺点 原有业务布署在虚拟机ecs kvm ，脚本分散，日志分散难于集中收集管理，监控无法统一，cpu、内存、磁盘资源得用率低，运维效率极低，无法集中管理。 新业务布署需要开通新的虚拟机，需要单独定制监控，各种crontab ,配置脚本，效率低下，ci-cd jenkins配置繁琐。 k8s容器化优势 利用k8s容器平台namespaces对不同环境进行区分,建产不同dev、test 、stage、prod环境,实现隔离。 通过容器化集中布署所有业务，实现一键布署所需环境业务。 统一集中监控报警所有容器服务异常状态。 统一集中收集所有服务日志至elk集群, 利用kibana面板进行分类，方便开发查日志。 基于k8s命令行二次开发，相关开发、测试人员、直接操作容器。 基于rbac对不同的环境授于不同的开发、测试访问k8s权限，防止越权。 通过jenkins 统一ci-cd编译发布过程。 项目容器化后, 整体服务器cpu、 内存、磁盘、资源利用减少%50，运维效率提高%60，原来需要N个运维做的事，现在一个人即可搞定。 k8s本身是一套分布式系统，要用好会遇到很多问题，不是说三天两头就能搞定，需要具备网络、linux系统、存储，等各方面专业知识，在使用过程中我们也踩了不少坑, 我们是基于二进制方试安装，我们k8s版本为1.10，经过一段时间的实践，k8s对于我们整个开发、测试、发布、运维流程帮助非常大，值得大力推广。 网络方案选择 flanneld vxlan udp以及 hsot-gw 所有节点同步路由 ，使用简单，方便，稳定，k8s入门首选。 calico 基于BGP协议的路由方案，支持acl ，部署复杂，出现问题难排查。 Weave UDP广播，本机建立新的BR，通过PCAP互通 ，国内使用比较少。 Open vSwitch UDP广播，本机建立新的BR，通过PCAP互通，openshift 以及混合云使用比较多。 我们对各个网络组件进行过调研对比，网络方案选择的是flanneld-hostgw+ipvs，在k8s1.9之前是不支持ipvs，kube-proxy负责所有svc规则的同步，使用的iptables,一个service会产生n条iptables记录。如果svc增加到上万条，iptables-svc同步会很慢，得几分钟，使用ipvs之后，所有节点的svc由ipvs lvs来负载，更快，更稳定。而且简单方便，使用门槛低， host-gw会在所有节同步路由表，每个容器都分配了一个IP地址，可用于与同一主机上的其他容器进行通信。对于通过网络进行通信，容器与主机的IP地址绑定。flanneld-hostgw性能接近calico，相对来说falnneld配置布署比calico简单很多。顺便提下flanneld-vxlan这种方式，需要通过udp封包解包，效率较低，适用于一些私有云对网络封包有限制，禁止路由路由表添加等有限制的平台。 flanneld 通过为每个容器提供可用于容器到容器通信的IP来解决问题。它使用数据包封装来创建跨越整个群集的虚拟覆盖网络。更具体地说，flanneld为每个主机提供一个IP子网（默认为/ 24），Docker守护程序可以从中为每个主机分配IP。 flannel使用etcd来存储虚拟IP和主机地址之间的映射。一个flanneld守护进程在每台主机上运行，并负责维护ETCD信息和路由数据包。 在此提一下，在使用flannled使用过程中遇到过严重bug 即租约失效，flanneld会shutdown 节点 网络组件，节点网络直接崩掉，解决办法是设置永久租期：https://coreos.com/flannel/docs/latest/reservations.html#reservations 传统业务迁移至k8s遇到的问题和痛点，devops遇到问题？ 使用k8s会建立两套网络，服务之间调用通过svc域名，默认网络、域名和现有物理网络是隔离的，开发，测试，运维无法像以前一样使用虚拟机一样，postman ip+端口 调试服务， 网络都不通，这些都是问题。 pod网络 和物理网络不通，windows办公电脑、linux虚拟机上现有的业务和k8s是隔离的。 svc网络 和物理网络不通，windows办公电脑、linux虚拟机上现有的业务和k8s是隔离的。 svc域名和物理网络不通，windows办公电脑、linux虚拟机上现有的业务和k8s是隔离的。 原有nginx 配置太多的location 几百层，不好迁移到ingress-nginx，ingress只支持简单的规则。 svc-nodeport访问，在所有node上开启端口监听，占用node节点端口资源，需要记住端口号。 ingress http 80端口， 必需通过域名引入，ingress http 80端口必需通过域名引入，原来简单nginx的location可以通过ingress引入。 tcp–udp–ingress tcp udp 端口访问需要配置一个ingress lb，很麻烦，要先规划好lb节点同样也需要仿问lb端口。 原有业务不能停，继续运行，同时要能兼容k8s环境,和k8s集群内服务互相通讯调用，网络需要通。 传统虚拟机架构我们只需要一个地址+端口直接访问调试各种服务，k8s是否能做到不用改变用户使用习惯，无感知使用呢？答案是打通devops全链路，像虚拟机一样访部k8s集群服务 , 我们打通k8s网络和物理网理直通，物理网络的dns域名直接调用k8s-dns域名服务直接互访，所有服务互通。公司原有业务和现有k8s集群无障碍互访。 配置一台k8s node节点机做路由转发，配置不需要太高，布署成路由器模式,所有外部访问k8s集群流量经该节点, 本机ip: 192.168.2.71 。 vim /etc/sysctl.conf net.ipv4.ip_forward = 1 设置全网路由通告,交换机或者linux、windows主机加上静态路由，打通网络。 route add -net 172.20.0.0 netmask 255.255.0.0 gw 192.168.2.71 route add -net 172.21.0.0 netmask 255.255.0.0 gw 192.168.2.71 增加dns服务器代理，外部服务需要访问k8s svc域名，首先需要解析域名，k8s服务只对集群内部开放，此时需要外部要能调用kube-dns 53号端口，所有办公电脑，业务都来请求kube-dns肯定撑不住 ，实时上确实是撑不住，我们做过测试，此时需要配置不同的域名进行分流策略，公网域名走公网dns,内部.svc.cluster.local走kube-dns。 建立dns代理服务器，ingress建立一个nginx-ingress服务反代kube-dns,ingress-nginx绑定到dns节点运行，在节点上监听 dns 53 端口。 [root@master1 kube-dns-proxy-1.10]# cat tcp-services-configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: tcp-services namespace: ingress-nginx data: 53: “kube-system/kube-dns:53” [root@master1 kube-dns-proxy-1.10]# cat udp-services-configmap.yaml kind: ConfigMap apiVersion: v1 metadata: name: udp-services namespace: ingress-nginx data: 53: “kube-system/kube-dns:53” [root@master1 kube-dns-proxy-1.10]# cat ingress-nginx-deploy.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-ingress-controller-dns namespace: ingress-nginx spec: replicas: 1 selector: matchLabels: app: ingress-nginx-dns template: metadata: labels: app: ingress-nginx-dns annotations: prometheus.io/port: ‘10254’ prometheus.io/scrape: ‘true’ spec: hostNetwork: true serviceAccountName: nginx-ingress-serviceaccount containers: - name: nginx-ingress-controller-dns image: registry-k8s.novalocal/public/nginx-ingress-controller:0.12.0 args: - /nginx-ingress-controller - —default-backend-service=$(POD_NAMESPACE)/default-http-backend # - —configmap=$(POD_NAMESPACE)/nginx-configuration - —tcp-services-configmap=$(POD_NAMESPACE)/tcp-services - —udp-services-configmap=$(POD_NAMESPACE)/udp-services - —annotations-prefix=nginx.ingress.kubernetes.io env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace ports: - name: http containerPort: 80 #- name: https # containerPort: 443 livenessProbe: failureThreshold: 3 httpGet: path: /healthz port: 10254 scheme: HTTP initialDelaySeconds: 10 最简单快捷的方式是安装dnsmasq，当然你也可以用bind ,powerdn,croedn等改造，上游dns配置为上一步骤增加nginx-ingress dns的地址，所有办公，业务电脑全部设置dns为此机,dnsmasq.conf 配置分流策略。 no-resolv server=/local/192.168.1.97 server=114.114.114.114 完成以上步骤 k8s podip svcip svc域名和办公，现有ecs、虚拟机完美融合，无缝访问，容器网络问题搞定。 windows访问k8s svc畅通无组，开发测试，完美无缝对接。 k8s 日志方案 普通虚拟机日志分散，难管理，需要登陆虚拟机一个个查看，k8s-docker可以很方便帮我们收集管理日志，日志方案有几种。 应用打到docker stdout 前台输出，docker输出到/var/lib/containers, 通过filebeat、fluentd 、daemonsets组件收集，这种对于小量日志还可以，大量日志性能很差，写入很慢. pod挂载host-path 把日志打到宿主机，宿主机启动filebeat， fluentd 、daemonsets 收集,无法判断来自哪个容器，pod namespaces。 pod的yml中定义两个 container ,同时启动一个附加的filebeat，两个container挂载一个共享卷来收集日志 我们用的第三种方案，通过一个附加容器filebeat来收集所有日志, filebeat–kakfa–logstash–es,自定义编译filebeat 镜相，为filebeat打上podip空间svc名等标签，方便识别来自哪个容器，哪个namespace， filebeat----kafkacluster-----logstash----es ilebeat收集日志打上关键字标签，namespace ，svc，podip 等 kibana 集中日志展示，建立dashboard分类，用户可以按namespce 分类不同环境，过滤选择查看不同模块的应用日志 简化kubectl 命 令, 提供给研发团队使用。实际上这里功能和jenkins以及kibana上是重复的，但是必需考虑到所有团队成员的使用感受，有人喜欢命令行，有人喜欢界面，简单好用就够。我打个比方，比如看日志，有人可能喜欢用命令行tail -f 看日志，用grep过滤等，有人喜欢用kibana看，那怎么办？于就有了两种方案，喜欢用图形界面的就去jenkins或kibana，你想用命令行的就给你命令行，满足你一切需求。统一集中通过指定的机器提供给开发、测试、运维、使用，方便调试，排障。通过统一的入口可以直接对容器进行服务创建，扩容，重启，登陆，查看日志，查看java启动参数 等，方便整个团队沟通。 在这里我们通过k8s rbac 授权身份认证 生产证书key kube-config key，授于不同项目组不同的管理权限，不同的项目组只有自己项目的权限，权限做了细分，不同研发、测试团队互不干扰。 Copyright © Mars丶小石头 2018 all right reserved，powered by Gitbook该文件修订时间： 2018-10-31 00:28:01 "}}